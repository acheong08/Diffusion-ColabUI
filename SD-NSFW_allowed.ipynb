{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acheong08/NovelAI-Colab/blob/main/NSFW_Disabled_NOP's_Stable_Diffusion_Colab_v0_54_(1_4_Weights).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# NSFW Disabled: NOP & WAS's Stable Diffusion Colab v0.54 (1.4 Weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stablity.AI Model Terms of Use\n",
        "By using this Notebook, you agree to the following Terms of Use, and license\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "\n",
        "You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "\n",
        "CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "\n",
        "You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license"
      ],
      "metadata": {
        "id": "fSanMIbKMydc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6RXjS1tTji"
      },
      "source": [
        "#Info\n",
        "\n",
        "Newest version can be found at: https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17\n",
        "\n",
        "WAS will be developing this colab with me, he has been doing great work! The logical thing would be to include him here, throw him some kudos for the great work done when you see him! :)\n",
        "\n",
        "Trying to make this a one-stop shop for various programs + a GOTO guide on how to install everything locally. If you have suggestions, bug reports, or implementations, feel free to contact me on Discord and/or leave a comment via colab's comment feature. NOP#1337\n",
        "\n",
        "\n",
        "Changelog:\n",
        "- v0.5: Did a complete code overhaul to make it a lot cleaner & to be a little bit more memory cognizant. It will also help me integrate new features. Also, working on CLIP Guidance, but having problems with it. Integration should be soon, it's definitely next on my Agenda.\n",
        "- v0.51: CLIP guidance is now available as a MODE\n",
        "- v0.52: Added img2img postprocessing. CAUTION: It's very error prone. If it runs into an error you need to click on \"Runtime\" and then on \"Restart Runtime\" or click \"Restart Runtime\" in the colab\n",
        "- v0.53: Added a bunch of user models. Not tested, but the option is there\n",
        "- v0.531: Some memory management\n",
        "- v0.54: Added IMG2IMG as an option to be the sole upscaler\n",
        "\n",
        "By NOP#1337 & WAS#0263\n",
        "\n",
        "img2img post-processing:\n",
        "\n",
        "Original Result + Upscaled:\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/1022133535945019392/1022543888021459074/unknown.png)\n",
        "\n",
        "Post Processed with IMG2IMG:\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/1022133535945019392/1022543749835919450/unknown.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeIWggi6TGH0"
      },
      "source": [
        "#Scheduler/Sampler Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYJ8pdUoTilg"
      },
      "source": [
        "Kudos to scarletpenn#7121 !\n",
        "\n",
        "![](https://cdn.discordapp.com/attachments/1002602742667280404/1014634578226462740/K-LMS_vs_PNDM_vs_DDIM_0-1.0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPyJJ2z-RJB7"
      },
      "source": [
        "#Changelog/Credits/FAQ/TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2uscKB2ROiU"
      },
      "source": [
        "Changelog:\n",
        "\n",
        "- v0.1: Colab creation\n",
        "- v0.11: Google Drive option for TXT2IMG and some error corrections\n",
        "- v0.12: Added more options to TXT2IMG\n",
        "- v0.13: Diffusers added a feature which broke the pipeline with the current implementation, reverted back to an older version\n",
        "- v0.14: Added in full precision in the diffuser method\n",
        "- v0.15: Added in file saving in drive for diffusers\n",
        "- v0.16: Added in prompt saving\n",
        "- v0.17: Added in the new weights and disabled the NSFW check\n",
        "- v0.18: Minor adjustments and more details saved in prompt saving\n",
        "- v0.19: Added in modifier experiments in Diffusers + example. More options to experiment will come with future updates\n",
        "- v0.20: Low VRAM patch is fixed. Getting 10 it/s with it on with a V100\n",
        "- v0.21: Diffusers now has an upscaler (Real-ESRGAN) <-- just updated to GFPGAN\n",
        "- v0.22: Added in a small little fun randomizer\n",
        "- v0.23: Now I support both upscalers. GFP is really good at faces but kind of sucks at upscaling. If you want the best of two worlds choose \"Both\" as an upscaler. T4 may have problems with one or both of them, looking at a fix for that (May get lucky with Real-ESRGAN).\n",
        "- v0.30: A complete code overhaul by WAS#0263 and a bunch of stuff added. With an overhaul, there could also different bugs. Shoot me ( NOP#1337 ) a discord message when you find one and tell WAS that he is awesome when you see him! If there are major bugs, I'll fix them as soon as I can\n",
        "- v0.31: Forgot to mention last update: No more huggingface login, that's all built-in now. Also, we have new facial enhancement. --> CodeFormer. It's like GFP but not as strong + with a nifty slider\n",
        "- v0.32: Added options for samplers (still having problems with other ones). Also added an option to sharpen the image.\n",
        "- v0.33: Added ddim & ETA for DDIM. Also trying to dim down some more VRAM\n",
        "- v0.34: Merged setup into the render cell.\n",
        "- v0.35: Just some backend stuff. Will behave differently, don't be alarmed. Sometimes VRAM got stuck and this should fix it. Also added a 'SKIP_PREVIEW' button to see if this can fix connectivity issues after running the colab.\n",
        "- v0.36: IMG2IMG (inits) are now available. If you like drawing, then turn on USE_INIT and empty out INIT_IMAGE\n",
        "- v0.37: Better drawing feature if no init image is provided\n",
        "- v0.38: Added options for weights and diffuser versions\n",
        "- v0.39: Added a (bad) first integration of inpainting. Plans are to have a full-on editor for masks similarly to img2img/init. You need to run diffusers first in order to set up the environment though. When you are done, make sure to clean up the vram in order to help prevent Out Of Memory errors\n",
        "- v0.4: Inpainting now supported within the main diffusers cell. Additionally, if there is no mask image provided, you can draw your own within the colab (Mobile unfortunately not supported YET)\n",
        "- v0.41: Added a `VRAM_OVER_SPEED` implementation that is based on a discussion in the diffusers Github. Checking this option will prioritize VRAM usage over creation speed. That being said, haven't tested it extensively\n",
        "- v0.42: added waifu as a model thanks to Alamgir#1781. Use at your own risk, haven't tested it. If you want to use it, just select it under `MODEL_ID`. It is trained on danbooru data therefore if you want to generate an anime character with a japanese name its better to put the last name first ((Since this is 0.42 does that mean that Waifu is the meaning of life?)))\n",
        "\n",
        "\n",
        "Credits:\n",
        "- WAS#0263 for giving great advice, coding tips, code, and recommendations. A MASSIVE help overhauling this thing\n",
        "- ð“‘ð“µð“ªð“·ð“¬ð““ð“®ð“žð“¯ð“¯ð“²ð“¬ð“²ð“ªð“µ#2485 for inspiring me to put an upscaler in the colab and for bug hunting\n",
        "- Gecktendo#8043 for helping with the default prompt\n",
        "- Original TXT2IMG Notebook: Lucas Ferreira da Silva, Madams, Greg Turk\n",
        "- Nee#9981, Alamgir#1781, and Queen#0613 for breaking my stuff bug hunting!\n",
        "\n",
        "FAQ:\n",
        "\n",
        "Q: What is the difference between Diffusers and TXT2IMG?\n",
        "\n",
        "A: Diffusers is the Huggingface Python Library and TXT2IMG is from the Stability AI Github. They both do the same thing, but differently. Whichever you want to use is just personal preference.\n",
        "\n",
        "Q: Which one should I use?\n",
        "\n",
        "A: Really just personal preference. For me: Right now I am heavily concentrating on diffusers just because it's a tad easier to work with.\n",
        "\n",
        "TODO:\n",
        "- Saving weights in Google Drive\n",
        "- Option to load a config file to load in preset settings\n",
        "- Support/Bugfixing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Donate"
      ],
      "metadata": {
        "id": "h-qNQtzBRbjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally did not want to include this, but people keep on asking me if they could donate. So if you want to fuel my caffeeine addiction: https://www.buymeacoffee.com/NOP1337 . 100% of the donations will go towards coffee\n",
        "\n",
        "Don't feel obligated to, this will always remain free no matter what"
      ],
      "metadata": {
        "id": "PmIPmy4jRegy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5V9MWbFtpNi"
      },
      "source": [
        "#GPU Info\n",
        " (If it throws an error here, go to Runtime, then click \"Change Runtime Type\" and then select \"GPU\"). There's also a chance that Colab put you on a GPU timeout if this is set and it still throws an error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ekR-LW6trWG"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHaZZ0uKti1c"
      },
      "source": [
        "# Diffusers Method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUlgKvW_EfqX"
      },
      "source": [
        "If anyone having problems running this colab on mobile device then try checking the `Desktop Site` in chrome from the menu from top right corner . (Kudos to Rohan Singh)\n",
        "\n",
        "Textual Inversion if you want to try it:\n",
        "\n",
        "Training: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\n",
        "\n",
        "Inference: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb\n",
        "\n",
        "If you're lazy and just want a text prompt with upscaling. No complicated settings:\n",
        "https://colab.research.google.com/drive/19kbftgwyq2yWnE6ZVukpzJ8QSXbRNatB?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New code below. Documentation sucks as of right now and if you are having problems with it, then you can find the old code under \"Legacy Version\". Currently there WILL be issues. But it should be a lot snappier"
      ],
      "metadata": {
        "id": "V8Y7vQ4TFNNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucr5_i21xSjv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Render Images\n",
        "class Cleaner:\n",
        "  def clean_env():\n",
        "    gc, torch = Manager.manage_imports('clean_env')\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "class Colab:\n",
        "  def __init__(self):\n",
        "    self.settings = self.UserSettings.set_settings()\n",
        "\n",
        "  def clear():\n",
        "    from IPython.display import clear_output; clear_output()\n",
        "\n",
        "  def manage_drive(drive_pic_dir):\n",
        "    exists, mount, makedirs = Manager.manage_imports('manage_drive')\n",
        "    if not exists('/content/drive'):\n",
        "      mount('/content/drive')\n",
        "    if not exists(f'/content/drive/MyDrive/{drive_pic_dir}'):\n",
        "      makedirs(f'/content/drive/MyDrive/{drive_pic_dir}')\n",
        "\n",
        "  class Images:\n",
        "    def resize_image():\n",
        "      pass\n",
        "\n",
        "    def suggest_resolution():\n",
        "      pass\n",
        "      \n",
        "    class Painter:\n",
        "      def inpaint(width, height):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        def draw(filename='drawing.png', color=\"white\", w=256, h=256, line_width=50,loop=False, init_img=\"init.jpg\"):\n",
        "          filename=\"init.jpg\"\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"100\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "        }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width};\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          \n",
        "          var button = document.querySelector('#save')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          reload_img_button.click()\n",
        "        \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "\n",
        "              var c = ctx\n",
        "              var imageData = ctx.getImageData(0,0, {w}, {h});\n",
        "              var pixel = imageData.data;\n",
        "              var r=0, g=1, b=2,a=3;\n",
        "            for (var p = 0; p<pixel.length; p+=4)\n",
        "            {{\n",
        "              if (\n",
        "                  pixel[p+r] != 255 &&\n",
        "                  pixel[p+g] != 255 &&\n",
        "                  pixel[p+b] != 255) \n",
        "              {{pixel[p+r] =0; pixel[p+g]=0; pixel[p+b]=0}}\n",
        "            }}\n",
        "\n",
        "            c.putImageData(imageData,0,0);\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(\"init_mask.png\", 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "\n",
        "\n",
        "\n",
        "        draw(filename = \"init_mask.png\", w=width, h=height)\n",
        "        import PIL.Image\n",
        "        return PIL.Image.open('init_mask.png')\n",
        "      def img2img(width, height):\n",
        "        import os\n",
        "        os.chdir('/content/')\n",
        "        def draw(filename='drawing.png', color=\"black\", bg_color=\"transparent\",w=256, h=256, line_width=1,loop=False):\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "        <div>\n",
        "          <label for=\"strokeColor\">Stroke</label>\n",
        "          <input type=\"color\" value=\"{color}\" id=\"strokeColor\">\n",
        "        \n",
        "          <label for=\"bgColor\">Background</label>\n",
        "          <input type=\"color\" value=\"{bg_color}\" id=\"bgColor\">\n",
        "        </div>\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"35\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "          <button id=\"exit\">Exit</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "      }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width}\n",
        "          ctx.fillStyle = \"{bg_color}\";\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "          var strokeColor = document.querySelector('#strokeColor')\n",
        "          var bgColor = document.querySelector('#bgColor')\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          bgColor.addEventListener(\"change\", updateBG, false);\n",
        "          strokeColor.addEventListener(\"change\", updateStroke, false);\n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          var button = document.querySelector('#save')\n",
        "          var exit_button = document.querySelector('#exit')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            exit_button.onclick = ()=>{{\n",
        "            resolve()\n",
        "          }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          // window.onload = async ()=>{{\n",
        "          //   console.log(\"loaded\")\n",
        "          //   let img = await loadImage('{html_filename}');  \n",
        "          //   ctx.drawImage(img, 0, 0);\n",
        "          // }}\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(filename, 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "        \n",
        "        draw(filename = \"custom_image.png\", w=width, h=height, bg_color=\"blue\", line_width=10)\n",
        "        import PIL.Image\n",
        "        return PIL.Image.open(\"/content/custom_image.png\")\n",
        "\n",
        "  class UserSettings:\n",
        "\n",
        "    def set_settings():\n",
        "      MODE = \"PROMPT\" #@param [\"PROMPT\", \"CLIP GUIDED PROMPT\", \"IMG2IMG\",\"Inpainting\",\"PROMPT FILE\"]\n",
        "      #@markdown `MODE` Select what mode you want to use <br>\n",
        "\n",
        "      #@markdown ---\n",
        "      settings = {\"mode\":MODE}\n",
        "      #@markdown GENERAL SETTINGS\n",
        "\n",
        "      WIDTH = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "      HEIGHT = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "      SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "      SEED = 0 #@param {type:'integer'}\n",
        "      settings[\"seed\"] = SEED \n",
        "      SCHEDULER = 'default' #@param [\"default\", \"pndm\", \"k-lms\", \"ddim\", 'ddim clip sampled']\n",
        "      settings[\"scheduler\"] = SCHEDULER \n",
        "\n",
        "      if SCHEDULER == 'ddim':\n",
        "        DDIM_ETA = 0.72 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "        settings[\"ddim_eta\"] = DDIM_ETA \n",
        "      PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "      settings[\"precision\"] = PRECISION\n",
        "      settings[\"width\"] = WIDTH\n",
        "      settings[\"height\"] = HEIGHT\n",
        "      settings[\"scale\"] = SCALE\n",
        "      IMG2IMG_POSTPROCESS = False #@param {type:'boolean'}\n",
        "      #@markdown `IMG2IMG_POSTPROCESS`: Postprocess the image with img2img after it's done. Will take the img2img settings to do the postprocessing, so make sure to change those if you set this. It can add a lot of detail to the final image after upscaling (sometimes it's hit or miss) although it is very slow since it needs to switch pipes<br>CAUTION: Very error-prone (will fix that down the road). If this runs into an error you need to clean the environment by clicking on \"Runtime\" and then on \"Restart Environment\"\n",
        "      settings[\"img2img_postprocess\"] = IMG2IMG_POSTPROCESS\n",
        "      \n",
        "      #@markdown ---\n",
        "\n",
        "      #@markdown UPSCALING SETTINGS\n",
        "\n",
        "      IMAGE_UPSCALER = \"GFPGAN + Enhanced ESRGAN\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\", \"IMG2IMG\"]\n",
        "      settings[\"image_upscaler\"] = IMAGE_UPSCALER \n",
        "\n",
        "      UPSCALE_AMOUNT = 2 #@param {type:\"raw\"}\n",
        "      settings[\"upscale_amount\"] = UPSCALE_AMOUNT \n",
        "\n",
        "      FIDELITY = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "      settings[\"codeformer_fidelity\"] = FIDELITY\n",
        "\n",
        "      SHARPEN_AMOUNT = 1 #@param{type:'slider', min:0, max:3, step:1}\n",
        "      settings[\"sharpen_amount\"] = SHARPEN_AMOUNT\n",
        "\n",
        "      \n",
        "      #@markdown ---\n",
        "\n",
        "      #@markdown MODE: PROMPT FILE SETTINGS\n",
        "      if MODE == \"PROMPT FILE\":\n",
        "        FILE_LOCATION = \"/content/diffusers_output/1663720628_prompt.json\" #@param {type:\"string\"}\n",
        "        settings['prompt_file'] = FILE_LOCATION\n",
        "\n",
        "      #@markdown ---\n",
        "\n",
        "      if MODE == \"PROMPT\":\n",
        "        #@markdown MODE: PROMPT SETTINGS\n",
        "        # PROMPT_TYPE = \"TEXT\" #@param [\"TEXT\",\"FILE\"]\n",
        "        # TODO\n",
        "        settings[\"prompt_type\"] = \"TEXT\"\n",
        "\n",
        "        if settings[\"prompt_type\"] == \"TEXT\":\n",
        "          TEXT_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:\"string\"}\n",
        "          settings[\"text_prompt\"] = TEXT_PROMPT\n",
        "        # elif PROMPT_TYPE == \"FILE\":\n",
        "        #   FILE_PROMPT = \"/content/prompt_file.txt\" #@param {type:\"string\"}\n",
        "        #   prompts = []\n",
        "        #   with open(FILE_PROMPT, 'r') as file:\n",
        "        #     for line in file.readlines():\n",
        "        #       prompts.append(line)\n",
        "        #   settings[\"file_prompt\"] = prompts\n",
        "        # TODO\n",
        "\n",
        "        PROMPT_STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "        settings[\"steps\"] = PROMPT_STEPS \n",
        "\n",
        "      #@markdown ---\n",
        "\n",
        "      elif MODE == \"CLIP GUIDED PROMPT\":\n",
        "        \n",
        "        #@markdown MODE: CLIP GUIDED PROMPT (still buggy and finicky, but works with autocast & low_vram_patch)<br>\n",
        "        #@markdown The VRAM tends to stick if this errors out. When that happens click on \"Runtime\" and then \"Restart Runtime\". It's very finicky, some settings work better than others.\n",
        "        CLIP_MODEL_ID = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\" #@param [\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\", \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", \"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\", \"openai/clip-vit-base-patch32\", \"openai/clip-vit-base-patch16\", \"openai/clip-vit-large-patch14\"] {allow-input: true}\n",
        "        settings[\"clip_model_id\"] = CLIP_MODEL_ID\n",
        "        CLIP_MODE_TEXT_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:\"string\"}\n",
        "        settings[\"text_prompt\"] = CLIP_MODE_TEXT_PROMPT\n",
        "        CLIP_GUIDANCE_PROMPT = \"\" #@param {type:\"string\"}\n",
        "        settings[\"clip_prompt\"] = CLIP_GUIDANCE_PROMPT\n",
        "        CLIP_MODE_STEPS = 200 #@param {type:\"integer\"}\n",
        "        settings[\"steps\"] = CLIP_MODE_STEPS\n",
        "        CLIP_MODE_SCALE = 13.7 #@param {type:\"raw\"}\n",
        "        settings[\"scale\"] = CLIP_MODE_SCALE\n",
        "        CLIP_GUIDANCE_SCALE = 100 #@param {type:\"raw\"}\n",
        "        settings[\"clip_guidance_scale\"] = CLIP_GUIDANCE_SCALE\n",
        "        CLIP_MODE_NUM_CUTOUTS = 64 #@param {type:\"raw\"}\n",
        "        settings[\"clip_cutouts\"] = CLIP_MODE_NUM_CUTOUTS\n",
        "        CLIP_UNFREEZE_UNET = True #@param {type:\"boolean\"}\n",
        "        settings[\"unfreeze_unet\"] = CLIP_UNFREEZE_UNET\n",
        "        CLIP_UNFREEZE_VAE = True #@param {type:\"boolean\"}\n",
        "        settings[\"unfreeze_vae\"] = CLIP_UNFREEZE_VAE\n",
        "\n",
        "    \n",
        "      elif MODE == \"Inpainting\":\n",
        "        #@markdown ---\n",
        "        \n",
        "        #@markdown MODE: Inpainting SETTINGS\n",
        "        INPAINT_PROMPT = \"A cat sitting on a bench\" #@param {type:\"string\"}\n",
        "        settings[\"text_prompt\"] = INPAINT_PROMPT\n",
        "        \n",
        "        INPAINT_IMAGE = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\" #@param {type:'string'}\n",
        "        settings[\"inpaint_image\"] = INPAINT_IMAGE\n",
        "        \n",
        "        MASK_IMAGE = \"\" #@param {type:'string'}\n",
        "        settings[\"mask_image\"] = MASK_IMAGE\n",
        "        \n",
        "        INPAINT_STRENGTH = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "        settings[\"inpaint_strength\"] = INPAINT_STRENGTH\n",
        "\n",
        "        INPAINT_MODE_STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "        settings[\"steps\"] = INPAINT_MODE_STEPS\n",
        "\n",
        "\n",
        "      if MODE == \"IMG2IMG\" or settings[\"img2img_postprocess\"] or settings['image_upscaler'] == 'IMG2IMG':\n",
        "        #@markdown ---\n",
        "        \n",
        "        #@markdown MODE: IMG2IMG SETTINGS\n",
        "        IMG_PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:\"string\"}\n",
        "        \n",
        "        INIT_IMAGE = \"https://raw.githubusercontent.com/dblunk88/txt2imghd/master/character_with_hat.jpg\" #@param {type: 'string'}\n",
        "        settings[\"init_image\"] = INIT_IMAGE\n",
        "        \n",
        "        INIT_STRENGTH = 0.6 #@param{type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "        IMG2IMG_MODE_STEPS=40 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "        \n",
        "        if settings[\"img2img_postprocess\"] or settings['image_upscaler'] == 'IMG2IMG':\n",
        "          if settings['mode'] == \"IMG2IMG\":\n",
        "            settings[\"text_prompt\"] = IMG_PROMPT\n",
        "            settings[\"init_strength\"] = INIT_STRENGTH\n",
        "            settings[\"steps\"] = IMG2IMG_MODE_STEPS\n",
        "          settings['img2img'] = {}\n",
        "          settings['img2img'][\"text_prompt\"] = IMG_PROMPT\n",
        "          settings['img2img'][\"init_strength\"] = INIT_STRENGTH\n",
        "          settings['img2img'][\"steps\"] = IMG2IMG_MODE_STEPS \n",
        "        else:\n",
        "          settings[\"text_prompt\"] = IMG_PROMPT\n",
        "          settings[\"init_strength\"] = INIT_STRENGTH\n",
        "          settings[\"steps\"] = IMG2IMG_MODE_STEPS \n",
        "\n",
        "\n",
        "      #@markdown ---\n",
        "      \n",
        "\n",
        "      #@markdown Version Settings\n",
        "      #https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
        "      MODEL_ID = \"CompVis/stable-diffusion-v1-4\" #@param [\"CompVis/stable-diffusion-v1-4\", \"CompVis/stable-diffusion-v1-3\",\"CompVis/stable-diffusion-v1-2\",\"CompVis/stable-diffusion-v1-1\",'hakurei/waifu-diffusion', 'ayan4m1/trinart_diffusers_v2', 'doohickey/trinart-waifu-diffusion-50-50', 'lambdalabs/sd-pokemon-diffusers', 'anton-l/ddpm-ema-pokemon-64'] {allow-input:true}\n",
        "      settings[\"model_id\"] = MODEL_ID\n",
        "\n",
        "      DIFFUSERS_VERSION = '91db81894b44798649b6cf54be085c205e146805' #@param [\"latest\", \"91db81894b44798649b6cf54be085c205e146805\", \"f3937bc8f3667772c9f1428b66f0c44b6087b04d\"]\n",
        "      settings[\"diffusers_version\"] = DIFFUSERS_VERSION\n",
        "      \n",
        "      #@markdown ---\n",
        "      \n",
        "      #@markdown ADVANCED SETTINGS\n",
        "      CLEAN_PREVIEW_AFTER_ITERS = 19 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "      settings[\"clean_iters\"] = CLEAN_PREVIEW_AFTER_ITERS\n",
        "\n",
        "      SKIP_BULKY_PREVIEWS = False #@param {type:'boolean'}\n",
        "      settings[\"bulky_skip\"] = SKIP_BULKY_PREVIEWS\n",
        "\n",
        "      KEEP_SEED = False #@param {type:'boolean'}\n",
        "      settings[\"keep_seed\"] = KEEP_SEED\n",
        "\n",
        "      NUM_ITERS = 4 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "      settings[\"num_iters\"] = NUM_ITERS\n",
        "\n",
        "      RUN_FOREVER = False #@param {type:\"boolean\"}\n",
        "      settings[\"run_forever\"] = RUN_FOREVER\n",
        "\n",
        "      SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "      settings[\"save_prompt_details\"] = SAVE_PROMPT_DETAILS\n",
        "\n",
        "      USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "      settings[\"use_drive_for_pics\"] = USE_DRIVE_FOR_PICS\n",
        "\n",
        "      \n",
        "\n",
        "      if USE_DRIVE_FOR_PICS:\n",
        "        DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "        settings[\"drive_pic_dir\"] = DRIVE_PIC_DIR\n",
        "\n",
        "      DELETE_ORIGINALS = True #@param{type:'boolean'}\n",
        "      settings[\"delete_originals\"] = DELETE_ORIGINALS\n",
        "\n",
        "      LOW_VRAM_PATCH = True #@param {type:\"boolean\"}\n",
        "      settings[\"low_vram_patch\"] = LOW_VRAM_PATCH\n",
        "\n",
        "      VRAM_OVER_SPEED = True #@param {type:\"boolean\"}\n",
        "      settings[\"vram_over_speed\"] = VRAM_OVER_SPEED\n",
        "\n",
        "      ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "      settings[\"enable_nsfw_filter\"] = ENABLE_NSFW_FILTER\n",
        "\n",
        "  \n",
        "\n",
        "      return settings\n",
        "\n",
        "class Upscalers:\n",
        "\n",
        "  def check_upscalers(settings,image):\n",
        "    Cleaner.clean_env()\n",
        "    if settings['image_upscaler'] == 'GFPGAN':\n",
        "      image = Upscalers.gfpgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'Enhanced Real-ESRGAN':\n",
        "      image = Upscalers.esrgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'GFPGAN + Enhanced ESRGAN':\n",
        "      image = Upscalers.gfpgan_esrgan(settings, image)\n",
        "    elif settings['image_upscaler'] == 'CodeFormer':\n",
        "      image = Upscalers.codeformer(settings, image)\n",
        "    elif settings['image_upscaler'] == 'CodeFormer + Enhanced ESRGAN':\n",
        "      image = Upscalers.codeformer_esrgan(settings, image)\n",
        "    Cleaner.clean_env()\n",
        "    return image\n",
        "\n",
        "  def gfpgan(settings, image):\n",
        "    import os, subprocess\n",
        "    if not os.path.exists('/content/GFPGAN/'):\n",
        "      Upscalers.Install.gfpgan()\n",
        "    os.chdir('/content/GFPGAN/')\n",
        "    print(subprocess.run(['mkdir', f'/content/GFPGAN/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['mkdir', f'/content/GFPGAN/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    image.save('/content/GFPGAN/temp/temp.png')\n",
        "    print(subprocess.run(['python',f'/content/GFPGAN/inference_gfpgan.py','-i', '/content/GFPGAN/temp/temp.png','-o','/content/GFPGAN/results/', '-w', f'{settings[\"codeformer_fidelity\"]}','-s',f'{settings[\"upscale_amount\"]}', '--bg_upsampler', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    import PIL.Image\n",
        "    image = PIL.Image.open(f'/content/GFPGAN/results/restored_imgs/temp.png')\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/GFPGAN/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/GFPGAN/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/')\n",
        "    return image\n",
        "\n",
        "  def esrgan(settings, image):\n",
        "    def closest_value(input_list, input_value):\n",
        "      difference = lambda input_list : abs(input_list - input_value)\n",
        "      res = min(input_list, key=difference)\n",
        "      return res\n",
        "    import os\n",
        "    if int(settings[\"upscale_amount\"]) not in [1,2,4,8]:\n",
        "      nearest_value = closest_value([2,4,8],settings[\"upscale_amount\"])\n",
        "      settings[\"upscale_amount\"] = nearest_value\n",
        "      print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "    if not os.path.exists('/content/Real-ESRGAN'):\n",
        "      Upscalers.Install.esrgan()\n",
        "    if not os.path.exists(f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth'):\n",
        "      import subprocess\n",
        "      print(subprocess.run(['wget',f'https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x{settings[\"upscale_amount\"]}.pth','-O',f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/Real-ESRGAN')\n",
        "    from realesrgan import RealESRGAN\n",
        "    import torch\n",
        "    model = RealESRGAN(torch.device('cuda'), scale = settings[\"upscale_amount\"])\n",
        "    model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{settings[\"upscale_amount\"]}.pth')\n",
        "    import numpy as np\n",
        "    image = model.predict(np.array(image))\n",
        "    os.chdir('/content/')\n",
        "    model = None\n",
        "    Cleaner.clean_env()\n",
        "    return image\n",
        "\n",
        "  def codeformer(settings, image):\n",
        "    import os, subprocess\n",
        "    if not os.path.exists('/content/CodeFormer/'):\n",
        "      Upscalers.Install.codeformer()\n",
        "    print(subprocess.run(['mkdir', f'/content/CodeFormer/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['mkdir', f'/content/CodeFormer/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    image.save('/content/CodeFormer/temp/temp.png')\n",
        "    os.chdir('/content/CodeFormer/')\n",
        "    print(subprocess.run(['python',f'inference_codeformer.py','--w', f'{settings[\"codeformer_fidelity\"]}','--test_path','/content/CodeFormer/temp','--upscale',f'{settings[\"upscale_amount\"]}', '--bg_upsampler', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    import PIL.Image\n",
        "    image = PIL.Image.open(f'/content/CodeFormer/results/temp_{settings[\"codeformer_fidelity\"]}/final_results/temp.png')\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/CodeFormer/results/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['rm', '-rf', f'/content/CodeFormer/temp/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/')\n",
        "    return image\n",
        "\n",
        "  def gfpgan_esrgan(settings, image):\n",
        "    orig_upscale = settings['upscale_amount']\n",
        "    settings['upscale_amount'] = 1\n",
        "    image = Upscalers.gfpgan(settings, image)\n",
        "    settings['upscale_amount'] = orig_upscale\n",
        "    image = Upscalers.esrgan(settings, image)\n",
        "    return image\n",
        "\n",
        "  def codeformer_esrgan(settings, image):\n",
        "    orig_upscale = settings['upscale_amount']\n",
        "    settings['upscale_amount'] = 1\n",
        "    image = Upscalers.codeformer(settings, image)\n",
        "    settings['upscale_amount'] = orig_upscale\n",
        "    image = Upscalers.esrgan(settings, image)\n",
        "    return image\n",
        "    \n",
        "\n",
        "  class Install:\n",
        "\n",
        "    def gfpgan():\n",
        "      import subprocess\n",
        "      print(subprocess.run(['git','clone','https://github.com/TencentARC/GFPGAN.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','basicsr'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','facexlib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      import os\n",
        "      os.chdir('/content/GFPGAN')\n",
        "      print(subprocess.run(['pip','install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['wget','https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth', '-P', '/content/GFPGAN/experiments/pretrained_models'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "\n",
        "    def esrgan():\n",
        "      import subprocess, os\n",
        "      print(subprocess.run(['git','clone','https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('Real-ESRGAN')\n",
        "      print(subprocess.run(['git','reset', '--hard','2a5afd04a0e43956d1640db00d3a528ca5972fd2'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','-r','/content/Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "      \n",
        "    def codeformer():\n",
        "      import subprocess\n",
        "      import os\n",
        "      print(subprocess.run(['git','clone','https://github.com/sczhou/CodeFormer.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['pip','install','-r','/content/CodeFormer/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/CodeFormer/')\n",
        "      print(subprocess.run(['python','basicsr/setup.py','develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','scripts/download_pretrained_models.py','facelib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      print(subprocess.run(['python','scripts/download_pretrained_models.py','CodeFormer'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      os.chdir('/content/')\n",
        "\n",
        "class Cache:\n",
        "  class Pipe:\n",
        "    def __init__(self, settings):\n",
        "      global pipe\n",
        "      global pipetype\n",
        "      try:\n",
        "        if pipetype != settings[\"mode\"] or pipe is None:\n",
        "          self.make(settings)\n",
        "      except NameError:\n",
        "        self.make(settings)\n",
        "      Manager.Diffusion.Scheduler.make(settings)\n",
        "      self.pipe = pipe\n",
        "      self.pipetype = settings_pipetype\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "\n",
        "      import math\n",
        "      from torch import einsum\n",
        "      try:\n",
        "        from einops import rearrange\n",
        "      except ModuleNotFoundError:\n",
        "        !pip install einops\n",
        "        from einops import rearrange\n",
        "      import types\n",
        "      from diffusers.models.attention import CrossAttention\n",
        "      import torch\n",
        "      batch_size, sequence_length, dim = x.shape\n",
        "\n",
        "      h = self.heads\n",
        "\n",
        "      q = self.to_q(x)\n",
        "      context = context if context is not None else x\n",
        "      k = self.to_k(context)\n",
        "      v = self.to_v(context)\n",
        "      del context, x\n",
        "\n",
        "      q = self.reshape_heads_to_batch_dim(q)\n",
        "      k = self.reshape_heads_to_batch_dim(k)\n",
        "      v = self.reshape_heads_to_batch_dim(v)\n",
        "\n",
        "      r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device)\n",
        "\n",
        "      stats = torch.cuda.memory_stats(q.device)\n",
        "      mem_total = torch.cuda.get_device_properties(0).total_memory\n",
        "      mem_active = stats['active_bytes.all.current']\n",
        "      mem_free = mem_total - mem_active\n",
        "\n",
        "      mem_required = q.shape[0] * q.shape[1] * k.shape[1] * 4 * 2.5\n",
        "      steps = 1\n",
        "\n",
        "      if mem_required > mem_free:\n",
        "          steps = 2**(math.ceil(math.log(mem_required / mem_free, 2)))\n",
        "\n",
        "      slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "      for i in range(0, q.shape[1], slice_size):\n",
        "          end = i + slice_size\n",
        "          s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n",
        "          s1 *= self.scale\n",
        "\n",
        "          s2 = s1.softmax(dim=-1)\n",
        "          del s1\n",
        "\n",
        "          r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "          del s2\n",
        "\n",
        "      del q, k, v\n",
        "\n",
        "      r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "      del r1\n",
        "\n",
        "      return self.to_out(r2)\n",
        "\n",
        "    def optimize_attention(model):\n",
        "        import types\n",
        "        from diffusers.models.attention import CrossAttention\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, CrossAttention):\n",
        "                module.forward = types.MethodType(Cache.Pipe.forward, module)\n",
        "\n",
        "    def make(settings):\n",
        "      global pipe \n",
        "      global pipetype\n",
        "      pipe = None\n",
        "      Cleaner.clean_env()\n",
        "      pipetype = settings['mode']\n",
        "      pipe_library = Manager.manage_imports(pipetype)\n",
        "      import os, subprocess, torch\n",
        "      username, token = Manager.Diffusion.creds()\n",
        "      subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "      left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "      right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      print('Making Pipe')\n",
        "      if settings['mode'] == \"CLIP GUIDED PROMPT\":\n",
        "        import torch\n",
        "        from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
        "        from PIL import Image\n",
        "        from transformers import CLIPFeatureExtractor, CLIPModel\n",
        "        import CLIP_GUIDED\n",
        "        def create_clip_guided_pipeline(\n",
        "            model_id=\"CompVis/stable-diffusion-v1-4\", clip_model_id=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", scheduler=\"plms\", low_vram_patch=True\n",
        "        ):\n",
        "            if low_vram_patch:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                  model_id,\n",
        "                  torch_dtype=torch.float16,\n",
        "                  revision=\"fp16\",\n",
        "                  use_auth_token=True,\n",
        "              )\n",
        "              clip_model = CLIPModel.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "              feature_extractor = CLIPFeatureExtractor.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "            else:\n",
        "              pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                  model_id,\n",
        "                  use_auth_token=True,\n",
        "              )\n",
        "              clip_model = CLIPModel.from_pretrained(clip_model_id)\n",
        "              feature_extractor = CLIPFeatureExtractor.from_pretrained(clip_model_id)\n",
        "\n",
        "            if settings[\"scheduler\"] == 'ddim clip sampled':\n",
        "              schedulers = Manager.manage_imports('ddim')\n",
        "            else:\n",
        "              schedulers = Manager.manage_imports(settings[\"scheduler\"])\n",
        "            if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n",
        "                                               num_train_timesteps=1000, skip_prk_steps=True)\n",
        "            elif settings[\"scheduler\"] == 'k-lms':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n",
        "                                                      num_train_timesteps=1000)\n",
        "            elif settings[\"scheduler\"] == 'ddim':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "            elif settings[\"scheduler\"] == 'ddim clip sampled':\n",
        "              scheduler = schedulers(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=True, set_alpha_to_one=False)\n",
        "            \n",
        "\n",
        "            guided_pipeline = CLIP_GUIDED.CLIPGuidedStableDiffusion(\n",
        "                unet=pipeline.unet,\n",
        "                vae=pipeline.vae,\n",
        "                tokenizer=pipeline.tokenizer,\n",
        "                text_encoder=pipeline.text_encoder,\n",
        "                scheduler=scheduler,\n",
        "                clip_model=clip_model,\n",
        "                feature_extractor=feature_extractor,\n",
        "            )\n",
        "\n",
        "            return guided_pipeline\n",
        "        \n",
        "        local_pipe = create_clip_guided_pipeline(settings[\"model_id\"], settings[\"clip_model_id\"], settings['scheduler'], settings[\"low_vram_patch\"])\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(local_pipe.unet)\n",
        "        local_pipe = local_pipe.to(\"cuda\")\n",
        "      else:\n",
        "        if settings[\"low_vram_patch\"]:\n",
        "          try:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "          except OSError:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "          if settings[\"mode\"] == \"PROMPT\":\n",
        "            del local_pipe.vae.encoder\n",
        "        else:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(local_pipe.unet)\n",
        "      pipe = local_pipe\n",
        "      local_pipe = None\n",
        "      Cleaner.clean_env()\n",
        "\n",
        "  \n",
        "\n",
        "class Manager:\n",
        "  def __init__(self):\n",
        "    self.colab = Colab()\n",
        "    self.cache = Cache()\n",
        "    from IPython.display import Javascript\n",
        "    display(Javascript(\"google.colab.output.resizeIframeToContent()\"))\n",
        "\n",
        "  def manage_imports(requester):\n",
        "    if requester == 'manage_drive':\n",
        "      from os.path import exists\n",
        "      from os import makedirs\n",
        "      from google.colab import drive\n",
        "      return exists, drive.mount, makedirs\n",
        "\n",
        "    elif requester == 'patch_nsfw':\n",
        "      from shutil import copyfile\n",
        "      from os import remove\n",
        "      return copyfile, remove\n",
        "\n",
        "    elif requester == 'general_diffusion_run':\n",
        "      import torch, sys\n",
        "      from random import randint\n",
        "      return torch, randint, sys\n",
        "\n",
        "    elif requester == \"prompt_run\":\n",
        "      pass\n",
        "\n",
        "    elif requester == \"img2img_run\":\n",
        "      pass\n",
        "    \n",
        "    elif requester == \"inpainter_run\":\n",
        "      pass\n",
        "    \n",
        "    elif requester == 'clean_env':\n",
        "      import gc, torch\n",
        "      return gc, torch\n",
        "\n",
        "    elif requester == \"PROMPT\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionPipeline\n",
        "      return StableDiffusionPipeline\n",
        "    \n",
        "    elif requester == \"IMG2IMG\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionImg2ImgPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionImg2ImgPipeline\n",
        "      return StableDiffusionImg2ImgPipeline\n",
        "\n",
        "    elif requester == \"Inpainting\":\n",
        "      try:\n",
        "        from diffusers import StableDiffusionInpaintPipeline\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from diffusers import StableDiffusionInpaintPipeline\n",
        "      return StableDiffusionInpaintPipeline\n",
        "\n",
        "    elif requester == \"CLIP GUIDED PROMPT\":\n",
        "      try: \n",
        "        from CLIP_GUIDED import CLIPGuidedStableDiffusion\n",
        "      except ModuleNotFoundError:\n",
        "        Manager.Diffusion.install_diffusers()\n",
        "        from CLIP_GUIDED import CLIPGuidedStableDiffusion\n",
        "      return CLIPGuidedStableDiffusion\n",
        "\n",
        "    elif requester == 'diffuser_install':\n",
        "      import subprocess, os\n",
        "      return subprocess, os\n",
        "\n",
        "    elif requester == 'default' or requester == 'pndm':\n",
        "      from diffusers.schedulers import PNDMScheduler\n",
        "      return PNDMScheduler\n",
        "    \n",
        "    elif requester == 'k-lms':\n",
        "      from diffusers.schedulers import LMSDiscreteScheduler\n",
        "      return LMSDiscreteScheduler\n",
        "\n",
        "    elif requester == 'ddim':\n",
        "      from diffusers.schedulers import DDIMScheduler\n",
        "      return DDIMScheduler\n",
        "\n",
        "\n",
        "\n",
        "  def eval_settings(self):\n",
        "    settings = self.colab.settings\n",
        "    if settings['mode'] == \"PROMPT FILE\":\n",
        "      with open(settings['prompt_file'],'r') as file:\n",
        "        import json\n",
        "        self.colab.settings = json.loads(file.read())\n",
        "        settings = self.colab.settings\n",
        "    import json\n",
        "    print(json.dumps(settings, indent=2))\n",
        "    global pipetype\n",
        "    global pipe\n",
        "    try:\n",
        "      if pipetype != settings['mode'] or pipe is None:\n",
        "        Cache.Pipe.make(settings)\n",
        "    except NameError:\n",
        "      Cache.Pipe.make(settings)\n",
        "    if settings[\"use_drive_for_pics\"]:\n",
        "      Colab.manage_drive(settings['drive_pic_dir'])\n",
        "    \n",
        "\n",
        "  class Diffusion:\n",
        "    def patch_nsfw(ENABLE_NSFW_FILTER):\n",
        "      copyfile, remove = Manager.manage_imports('patch_nsfw')\n",
        "      remove('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      if ENABLE_NSFW_FILTER:\n",
        "        copyfile(f'/content/safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      else:\n",
        "        copyfile(f'/content/safety_checker_patched.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "\n",
        "    def install_diffusers():\n",
        "      subprocess, os = Manager.manage_imports('diffuser_install')\n",
        "      settings = Colab.UserSettings.set_settings()\n",
        "      \n",
        "      print('Installing Transformers')\n",
        "      print(subprocess.run(['pip','install','transformers'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Installing Diffusers')\n",
        "      if settings['diffusers_version'] == 'latest':\n",
        "        print(subprocess.run(['pip','install','-U',f'git+https://github.com/huggingface/diffusers.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      else:\n",
        "        print(subprocess.run(['pip','install','-U',f'git+https://github.com/huggingface/diffusers.git@{settings[\"diffusers_version\"]}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Creating Directories')\n",
        "      print(subprocess.run(['mkdir','diffusers_output'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Installing Dependencies')\n",
        "      print(subprocess.run(['pip','install','pytorch-pretrained-bert','spacy','ftfy','scipy'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Populating Spacy')\n",
        "      print(subprocess.run(['python','-m','space','download','en'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      Colab.clear()\n",
        "      print('Logging into Huggingface')\n",
        "      username, token = Manager.Diffusion.creds()\n",
        "      subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "      left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "      right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "      # Manager.Diffusion.install_model(username, token, settings[\"model_id\"])\n",
        "      Colab.clear()\n",
        "      if not os.path.exists('/content/safety_checker_patched.py'):\n",
        "        print('Creating Patches')\n",
        "        print(subprocess.run(['cp','/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','/content/safety_checker.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['cp','/content/safety_checker.py','/content/safety_checker_patched.py'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        with open(f'/content/safety_checker_patched.py','r') as unpatched_file:\n",
        "          patch = unpatched_file.read().replace('for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):','#for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):').replace('if has_nsfw_concept:','# if has_nsfw_concept:').replace('images[idx] = np.zeros(images[idx].shape)  # black image', '# images[idx] = np.zeros(images[idx].shape)  # black image').replace(\"Potential NSFW content was detected in one or more images. A black image will be returned instead.\",\"Potential NSFW content was detected in one or more images. It's patched out, no actions were taken.\").replace(\" Try again with a different prompt and/or seed.\",\"\")\n",
        "        with open(f'/content/safety_checker_patched.py','w') as file:\n",
        "          file.write(patch)\n",
        "      with open('/content/CLIP_GUIDED.py', 'w') as file:\n",
        "        file.write('''\n",
        "import inspect\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from diffusers import AutoencoderKL, DiffusionPipeline, LMSDiscreteScheduler, PNDMScheduler, UNet2DConditionModel\n",
        "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipelineOutput\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPFeatureExtractor, CLIPModel, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cut_power=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cut_size = cut_size\n",
        "        self.cut_power = cut_power\n",
        "\n",
        "    def forward(self, pixel_values, num_cutouts):\n",
        "        sideY, sideX = pixel_values.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(num_cutouts):\n",
        "            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def set_requires_grad(model, value):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = value\n",
        "\n",
        "\n",
        "class CLIPGuidedStableDiffusion(DiffusionPipeline):\n",
        "    \"\"\"CLIP guided stable diffusion based on the amazing repo by @crowsonkb and @Jack000\n",
        "    - https://github.com/Jack000/glid-3-xl\n",
        "    - https://github.dev/crowsonkb/k-diffusion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        clip_model: CLIPModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler],\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            clip_model=clip_model,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "        self.normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "        self.make_cutouts = MakeCutouts(feature_extractor.size)\n",
        "\n",
        "        set_requires_grad(self.text_encoder, False)\n",
        "        set_requires_grad(self.clip_model, False)\n",
        "\n",
        "    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n",
        "        if slice_size == \"auto\":\n",
        "            # half the attention head size is usually a good trade-off between\n",
        "            # speed and memory\n",
        "            slice_size = self.unet.config.attention_head_dim // 2\n",
        "        self.unet.set_attention_slice(slice_size)\n",
        "\n",
        "    def disable_attention_slicing(self):\n",
        "        self.enable_attention_slicing(None)\n",
        "\n",
        "    def freeze_vae(self):\n",
        "        set_requires_grad(self.vae, False)\n",
        "\n",
        "    def unfreeze_vae(self):\n",
        "        set_requires_grad(self.vae, True)\n",
        "\n",
        "    def freeze_unet(self):\n",
        "        set_requires_grad(self.unet, False)\n",
        "\n",
        "    def unfreeze_unet(self):\n",
        "        set_requires_grad(self.unet, True)\n",
        "\n",
        "    @torch.enable_grad()\n",
        "    def cond_fn(\n",
        "        self,\n",
        "        latents,\n",
        "        timestep,\n",
        "        index,\n",
        "        text_embeddings,\n",
        "        noise_pred_original,\n",
        "        text_embeddings_clip,\n",
        "        clip_guidance_scale,\n",
        "        num_cutouts,\n",
        "        use_cutouts=True,\n",
        "    ):\n",
        "        latents = latents.detach().requires_grad_()\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "            latent_model_input = latents / ((sigma**2 + 1) ** 0.5)\n",
        "        else:\n",
        "            latent_model_input = latents\n",
        "\n",
        "        # predict the noise residual\n",
        "        noise_pred = self.unet(latent_model_input, timestep, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "        if isinstance(self.scheduler, PNDMScheduler):\n",
        "            alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
        "            beta_prod_t = 1 - alpha_prod_t\n",
        "            # compute predicted original sample from predicted noise also called\n",
        "            # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "            pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n",
        "\n",
        "            fac = torch.sqrt(beta_prod_t)\n",
        "            sample = pred_original_sample\n",
        "        elif isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            sample = latents - sigma * noise_pred\n",
        "        else:\n",
        "            raise ValueError(f\"scheduler type {type(self.scheduler)} not supported\")\n",
        "\n",
        "        sample = 1 / 0.18215 * sample\n",
        "        image = self.vae.decode(sample).sample\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        if use_cutouts:\n",
        "            image = self.make_cutouts(image, num_cutouts)\n",
        "        else:\n",
        "            image = transforms.Resize(self.feature_extractor.size)(image)\n",
        "        image = self.normalize(image)\n",
        "\n",
        "        image_embeddings_clip = self.clip_model.get_image_features(image).float()\n",
        "        image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        if use_cutouts:\n",
        "            dists = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip)\n",
        "            dists = dists.view([num_cutouts, sample.shape[0], -1])\n",
        "            loss = dists.sum(2).mean(0).sum() * clip_guidance_scale\n",
        "        else:\n",
        "            loss = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip).mean() * clip_guidance_scale\n",
        "\n",
        "        grads = -torch.autograd.grad(loss, latents)[0]\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents.detach() + grads * (sigma**2)\n",
        "            noise_pred = noise_pred_original\n",
        "        else:\n",
        "            noise_pred = noise_pred_original - torch.sqrt(beta_prod_t) * grads\n",
        "        return noise_pred, latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        height: Optional[int] = 512,\n",
        "        width: Optional[int] = 512,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        clip_guidance_scale: Optional[float] = 100,\n",
        "        clip_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_cutouts: Optional[int] = 4,\n",
        "        use_cutouts: Optional[bool] = True,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "    ):\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        if clip_guidance_scale > 0:\n",
        "            if clip_prompt is not None:\n",
        "                clip_text_input = self.tokenizer(\n",
        "                    clip_prompt,\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.tokenizer.model_max_length,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\",\n",
        "                ).input_ids.to(self.device)\n",
        "            else:\n",
        "                clip_text_input = text_input.input_ids.to(self.device)\n",
        "            text_embeddings_clip = self.clip_model.get_text_features(clip_text_input)\n",
        "            text_embeddings_clip = text_embeddings_clip / text_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # get the initial random noise unless the user supplied it\n",
        "\n",
        "        # Unlike in other pipelines, latents need to be generated in the target device\n",
        "        # for 1-to-1 results reproducibility with the CompVis implementation.\n",
        "        # However this currently doesn't work in `mps`.\n",
        "        latents_device = \"cpu\" if self.device.type == \"mps\" else self.device\n",
        "        latents_shape = (batch_size, self.unet.in_channels, height // 8, width // 8)\n",
        "        if latents is None:\n",
        "            latents = torch.randn(\n",
        "                latents_shape,\n",
        "                generator=generator,\n",
        "                device=latents_device,\n",
        "            )\n",
        "        else:\n",
        "            if latents.shape != latents_shape:\n",
        "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
        "        latents = latents.to(self.device)\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        if accepts_offset:\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # if we use LMSDiscreteScheduler, let's make sure latents are multiplied by sigmas\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "        for i, t in enumerate(self.progress_bar(self.scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                sigma = self.scheduler.sigmas[i]\n",
        "                # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "            # perform classifier free guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # perform clip guidance\n",
        "            if clip_guidance_scale > 0:\n",
        "                text_embeddings_for_guidance = (\n",
        "                    text_embeddings.chunk(2)[0] if do_classifier_free_guidance else text_embeddings\n",
        "                )\n",
        "                noise_pred, latents = self.cond_fn(\n",
        "                    latents,\n",
        "                    t,\n",
        "                    i,\n",
        "                    text_embeddings_for_guidance,\n",
        "                    noise_pred,\n",
        "                    text_embeddings_clip,\n",
        "                    clip_guidance_scale,\n",
        "                    num_cutouts,\n",
        "                    use_cutouts,\n",
        "                )\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                latents = self.scheduler.step(noise_pred, i, latents).prev_sample\n",
        "            else:\n",
        "                latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image, None)\n",
        "\n",
        "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
        "\n",
        "        ''')\n",
        "      Manager.Diffusion.patch_nsfw(settings['enable_nsfw_filter'])\n",
        "      Colab.clear()\n",
        "\n",
        "\n",
        "    def install_model(username, token, model_id):\n",
        "      subprocess, os = Manager.manage_imports('diffuser_install')\n",
        "      print('Installing Model')\n",
        "      print(subprocess.run(['git','lfs','install'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "      # This will take a while\n",
        "      print(subprocess.run(['git','lfs','clone',f'https://{username}:{token}@huggingface.co/{model_id}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "    def creds():\n",
        "      return 'x90', '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'\n",
        "\n",
        "    def img2img_init(settings):\n",
        "      def preprocess(image):\n",
        "        import numpy as np\n",
        "        import torch\n",
        "        import PIL.Image\n",
        "        w, h = image.size\n",
        "        w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "        image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "        image = np.array(image).astype(np.float32) / 255.0\n",
        "        image = image[None].transpose(0, 3, 1, 2)\n",
        "        image = torch.from_numpy(image)\n",
        "        return 2.*image - 1.\n",
        "      global pipe\n",
        "      import PIL.Image\n",
        "      import requests\n",
        "      if 'http' in settings['init_image']:\n",
        "        from io import BytesIO\n",
        "        response = requests.get(settings['init_image'])\n",
        "        init_image = PIL.Image.open(BytesIO(response.content))\n",
        "      elif settings['init_image'] is None or settings['init_image'] == \"\":\n",
        "        init_image = Colab.Images.Painter.img2img(settings['width'], settings['height'])\n",
        "      else:\n",
        "        #it's a file\n",
        "        init_image = PIL.Image.open(settings['init_image'])\n",
        "      print(\"Init Image (automatically resized to match user input)\")\n",
        "      init_image = init_image.resize((settings['width'], settings['height']))\n",
        "      display(init_image)\n",
        "      init_image = preprocess(init_image.convert(\"RGB\"))\n",
        "      return init_image\n",
        "    \n",
        "    def inpaint_init(settings):\n",
        "      import PIL.Image\n",
        "      def download(location):\n",
        "        from io import BytesIO\n",
        "        import requests\n",
        "        import PIL.Image\n",
        "        response = requests.get(location)\n",
        "        return PIL.Image.open(BytesIO(response.content))\n",
        "      if 'http' in settings[\"inpaint_image\"]:\n",
        "        init_image = download(settings[\"inpaint_image\"])\n",
        "      else:\n",
        "        init_image = PIL.Image.open(settings[\"inpaint_image\"])\n",
        "      init_image = init_image.resize((settings['width'], settings['height']))\n",
        "      if 'http' in settings[\"mask_image\"]:\n",
        "        mask_image = download(settings[\"mask_image\"])\n",
        "      elif settings[\"mask_image\"]:\n",
        "        mask_image = PIL.Image.open(settings[\"mask_image\"])\n",
        "      else:\n",
        "        init_image.save(\"init.jpg\")\n",
        "        mask_image = Colab.Images.Painter.inpaint(settings['width'], settings['height'])\n",
        "      mask_image.resize((settings['width'], settings['height']))\n",
        "      return init_image, mask_image\n",
        "\n",
        "    class Scheduler:\n",
        "      def make(settings):\n",
        "        scheduler = Manager.manage_imports(settings[\"scheduler\"])\n",
        "        global pipe\n",
        "        if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "        elif settings[\"scheduler\"] == 'k-lms':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif settings[\"scheduler\"] == 'ddim':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "    class Runner:\n",
        "\n",
        "      def run(settings):\n",
        "        def sharpen_mage(image, samples=1):\n",
        "          from PIL import ImageFilter\n",
        "          im = image\n",
        "          for i in range(samples):\n",
        "              im = im.filter(ImageFilter.SHARPEN)\n",
        "          return im\n",
        "        import time\n",
        "        torch, precision_scope, randint, sys = Manager.Diffusion.Runner.get_general_imports(settings)\n",
        "        with torch.no_grad():\n",
        "          with precision_scope(\"cuda\"):\n",
        "            if settings['seed'] == 0:\n",
        "              settings['seed'] = randint(0,sys.maxsize)\n",
        "            generator = torch.Generator(\"cuda\").manual_seed(settings['seed'])\n",
        "            counter = 1\n",
        "            clean_counter = 0\n",
        "            running = True\n",
        "            if settings[\"use_drive_for_pics\"]:\n",
        "              outdir = f'/content/drive/MyDrive/{settings[\"drive_pic_dir\"]}'\n",
        "            else:\n",
        "              outdir = '/content/diffusers_output'\n",
        "            epoch_time = int(time.time())\n",
        "            if settings[\"save_prompt_details\"]:\n",
        "              with open(f'{outdir}/{epoch_time}_prompt.json', 'w') as file:\n",
        "                import json\n",
        "                file.write(json.dumps(settings, indent=2))\n",
        "            if settings['mode'] == \"IMG2IMG\":\n",
        "              init_image = Manager.Diffusion.img2img_init(settings)\n",
        "            elif settings['mode'] == 'Inpainting':\n",
        "              init_image, mask_image = Manager.Diffusion.inpaint_init(settings)\n",
        "            while running:\n",
        "              Cleaner.clean_env()\n",
        "              if settings[\"mode\"] == \"PROMPT\":\n",
        "                if settings['prompt_type'] == 'TEXT':\n",
        "                  image = Manager.Diffusion.Runner.text_prompt(settings, torch, generator)\n",
        "                else:\n",
        "                  for prompt in settings[\"file_prompt\"]:\n",
        "                    pass\n",
        "                    # TODO\n",
        "              elif settings[\"mode\"] == \"IMG2IMG\":\n",
        "                image = Manager.Diffusion.Runner.img_to_img(settings, torch, generator, init_image)\n",
        "              elif settings[\"mode\"] == \"Inpainting\":\n",
        "                image = Manager.Diffusion.Runner.inpainting(settings, torch, generator, init_image, mask_image)\n",
        "              elif settings[\"mode\"] == \"CLIP GUIDED PROMPT\":\n",
        "                Cleaner.clean_env()\n",
        "                image = Manager.Diffusion.Runner.clip_guided_prompt(settings, torch, generator)\n",
        "                Cleaner.clean_env()\n",
        "              if settings['image_upscaler'] in ['None','IMG2IMG'] or not settings[\"delete_originals\"]:\n",
        "                image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_original.png')\n",
        "                epoch_time = int(time.time())\n",
        "              clean_counter += 1\n",
        "              if settings[\"clean_iters\"] <= clean_counter:\n",
        "                Colab.clear()\n",
        "                clean_counter = 0\n",
        "              print(f'Image {counter}. SEED: {settings[\"seed\"]}')\n",
        "              display(image)\n",
        "              print('Enhancing and Upscaling')\n",
        "              if settings['image_upscaler'] != 'None':\n",
        "                image = Upscalers.check_upscalers(settings,image)\n",
        "                if settings['image_upscaler'] == 'IMG2IMG':\n",
        "                  image = image.resize((settings['width']*settings[\"upscale_amount\"], settings['height']*settings[\"upscale_amount\"]))\n",
        "                if settings['sharpen_amount'] > 0:\n",
        "                  image = sharpen_mage(image, settings['sharpen_amount'])\n",
        "                  if not settings[\"bulky_skip\"]:\n",
        "                    display(image)\n",
        "                  image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}_sharpened_{settings[\"sharpen_amount\"]}.png')\n",
        "                  epoch_time = int(time.time())\n",
        "                else:\n",
        "                  if not settings[\"bulky_skip\"]:\n",
        "                    display(image)\n",
        "                  image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}.png')\n",
        "                  epoch_time = int(time.time())\n",
        "              if settings[\"img2img_postprocess\"] or settings['image_upscaler'] == 'IMG2IMG':\n",
        "                if settings['image_upscaler'] == 'IMG2IMG':\n",
        "                  image = image.resize((settings['width']*settings[\"upscale_amount\"], settings['height']*settings[\"upscale_amount\"]))\n",
        "                image = Manager.Diffusion.Runner.img2img_postprocess(settings, image, generator)\n",
        "                if not settings[\"bulky_skip\"]:\n",
        "                  display(image)\n",
        "                image.save(f'{outdir}/{epoch_time}_seed_{settings[\"seed\"]}_upscaled_{settings[\"upscale_amount\"]}_img2imgenhanced.png')\n",
        "              if not settings['keep_seed']:\n",
        "                settings['seed'] += 1\n",
        "                generator = torch.Generator(\"cuda\").manual_seed(settings['seed'])\n",
        "\n",
        "              if not settings['run_forever']:\n",
        "                if counter >= settings['num_iters']:\n",
        "                  running = False\n",
        "                  \n",
        "              counter += 1\n",
        "\n",
        "      def img2img_postprocess(settings, image, generator):\n",
        "        import torch\n",
        "        print(\"running img2img postprocessing. Switching to img2img pipe\")\n",
        "        global pipe\n",
        "        pipe = None\n",
        "        Cleaner.clean_env()\n",
        "        pipe_library = Manager.manage_imports(\"IMG2IMG\")\n",
        "        import os, subprocess, torch\n",
        "        username, token = Manager.Diffusion.creds()\n",
        "        subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "        left_of_pipe = subprocess.Popen([\"echo\", token], stdout=subprocess.PIPE)\n",
        "        right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if settings[\"low_vram_patch\"]:\n",
        "          try:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "          except OSError:\n",
        "            local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        else:\n",
        "          local_pipe = pipe_library.from_pretrained(settings['model_id'], use_auth_token=True).to(\"cuda\")\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          local_pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(local_pipe.unet)\n",
        "          \n",
        "        pipe = local_pipe\n",
        "        local_pipe = None\n",
        "        del local_pipe\n",
        "        Cleaner.clean_env()\n",
        "        scheduler = Manager.manage_imports(settings[\"scheduler\"])\n",
        "        if settings[\"vram_over_speed\"]:\n",
        "          pipe.enable_attention_slicing()\n",
        "          Cache.Pipe.optimize_attention(pipe.unet)\n",
        "        if settings[\"scheduler\"] == 'default' or settings[\"scheduler\"] == 'pndm':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "        elif settings[\"scheduler\"] == 'k-lms':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif settings[\"scheduler\"] == 'ddim':\n",
        "          pipe.scheduler = scheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "        img2img_settings = {\n",
        "            \"text_prompt\":settings['img2img'][\"text_prompt\"],\n",
        "            'init_strength':settings['img2img'][\"init_strength\"],\n",
        "            'scheduler':settings['scheduler'],\n",
        "            'scale':settings['scale'],\n",
        "            'steps':settings['img2img'][\"steps\"]\n",
        "        }\n",
        "        image = Manager.Diffusion.Runner.img_to_img(img2img_settings, torch, generator, image)\n",
        "        pipe = None\n",
        "        del pipe\n",
        "        print('Switching back to old pipe and then displaying the image')\n",
        "        Cleaner.clean_env()\n",
        "        Cache.Pipe.make(settings)\n",
        "        Manager.Diffusion.Scheduler.make(settings)\n",
        "        return image\n",
        "\n",
        "      def get_general_imports(settings):\n",
        "        torch, randint, sys = Manager.manage_imports('general_diffusion_run')\n",
        "        if settings['precision'] == 'autocast':\n",
        "          return torch, torch.autocast, randint, sys\n",
        "        else:\n",
        "          from contextlib import nullcontext\n",
        "          return torch, nullcontext, randint, sys\n",
        "\n",
        "      def text_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], width=settings['width'], height=settings['height'], guidance_scale=settings['scale'], eta=settings[\"ddim_eta\"], generator=generator)\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], width=settings['width'], height=settings['height'], guidance_scale=settings['scale'], generator=generator)\n",
        "        return image[\"sample\"][0]\n",
        "        \n",
        "\n",
        "      def file_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        pass\n",
        "\n",
        "      def img_to_img(settings, torch, generator, init_image):\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, strength=settings['init_strength'], eta=settings[\"ddim_eta\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, strength=settings['init_strength'], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        return image\n",
        "\n",
        "      def inpainting(settings, torch, generator, init_image, mask_image):\n",
        "        global pipe\n",
        "        if settings['scheduler'] == 'ddim':\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, mask_image=mask_image, strength=settings[\"inpaint_strength\"], eta=settings[\"ddim_eta\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        else:\n",
        "          image = pipe(prompt=settings['text_prompt'], num_inference_steps=settings['steps'], init_image=init_image, mask_image=mask_image, strength=settings[\"inpaint_strength\"], guidance_scale=settings['scale'], generator=generator)[\"sample\"][0]\n",
        "        return image\n",
        "\n",
        "      def clip_guided_prompt(settings, torch, generator):\n",
        "        global pipe\n",
        "        if settings[\"unfreeze_unet\"] == \"True\":\n",
        "          pipe.unfreeze_unet()\n",
        "        else:\n",
        "          pipe.freeze_unet()\n",
        "\n",
        "        if settings[\"unfreeze_vae\"] == \"True\":\n",
        "         pipe.unfreeze_vae()\n",
        "        else:\n",
        "          pipe.freeze_vae()\n",
        "        use_cutouts = False\n",
        "        if settings[\"clip_cutouts\"] >= 1:\n",
        "          use_cutouts = True\n",
        "        Cleaner.clean_env()\n",
        "        image = pipe(\n",
        "            settings[\"text_prompt\"],\n",
        "            clip_prompt=settings[\"clip_prompt\"] if settings[\"clip_prompt\"].strip() != \"\" else None,\n",
        "            num_inference_steps=settings[\"steps\"],\n",
        "            guidance_scale=settings[\"scale\"], \n",
        "            clip_guidance_scale=settings[\"clip_guidance_scale\"],\n",
        "            num_cutouts=settings[\"clip_cutouts\"],\n",
        "            use_cutouts=use_cutouts == \"True\",\n",
        "            generator=generator,\n",
        "            width=settings[\"width\"],\n",
        "            height=settings[\"height\"]\n",
        "        ).images[0]\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  Cleaner.clean_env()\n",
        "  manager = Manager()\n",
        "  # makes pipe and schedulers\n",
        "  manager.eval_settings()\n",
        "  # make images\n",
        "  Manager.Diffusion.Runner.run(manager.colab.settings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font color=\"orange\">**Clean Environment Up**</font>\n",
        "#@markdown <font size=\"3\">**Soft Reset** the environment by deleting pipes, models, and image handlers from memory. Use this when the VRAM gets stuck after changing pipes<br><br>\n",
        "#@markdown **Note:** Before using this cell, give a minute for the system itself to flush some stuff. This will give a higher chance of this function working.<br>\n",
        "#@markdown **Note 2:** Sometimes you'll get a persistent OOM bug when the GPU has been unallocated from your session. This is common with the new (09/2022) Free Colab Sessions</font>\n",
        "\n",
        "try:\n",
        "    pipe = None\n",
        "    del pipe;\n",
        "except NameError:\n",
        "    pass\n",
        "finally:\n",
        "    Cleaner.clean_env()"
      ],
      "metadata": {
        "id": "FxvIfgi9YH4a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font color=\"orange\">**Restart Runtime**</font>\n",
        "#@markdown Click this if `Clean Environment Up` does not work\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eueubD4xprzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Settings Explanation"
      ],
      "metadata": {
        "id": "uYuBa_URJvet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`MODE`: The type of image generation that you want to do. Look for the corresponding setting options, usually prefixed with MODE and then the mode name\n",
        "\n",
        "*GENERAL SETTINGS*\n",
        "\n",
        "`WIDTH`: The Width you want your image to have\n",
        "\n",
        "`HEIGHT`: The Height you want your image to have\n",
        "\n",
        "`SCALE`: How strong you want your prompt to influence your image\n",
        "\n",
        "`SEED`: Each number corresponds to a unique image. If you reuse the seed, it should reproduce the same image with the same settings & prompt. Can also be used to make minor adjustments to previous images\n",
        "\n",
        "`SCHEDULER`: Each scheduler creates an image differently and has a slightly different style\n",
        "\n",
        "`DDIM_ETA`: If DDIM scheduler is used, the ETA determines how much noise is generated each iteration\n",
        "\n",
        "`PRECISION`: FULL has better quality (but is incompatible with the LOW_VRAM_PATCH, but autocast is quicker and does not use as much VRAM\n",
        "\n",
        "*MODE: PROMPT FILE SETTINGS*\n",
        "\n",
        "`FILE_LOCATION`: The location of a prompt .json file from a previous run (from the new version)\n",
        "\n",
        "*MODE: PROMPT SETTINGS*\n",
        "\n",
        "`TEXT_PROMPT`: The text prompt of the image that you want to generate\n",
        "\n",
        "`STEPS`: How many iterations you want to use. 50 should have decent images, 200 is a sweet spot between quality and time.\n",
        "\n",
        "*MODE: CLIP GUIDED PROMPT*\n",
        "\n",
        "This seems to be more creative than the regular mode\n",
        "\n",
        "Click here to learn more about the models: https://laion.ai/blog/large-openclip/\n",
        "\n",
        "`CLIP_MODEL_ID`: The CLIP model that you want to use\n",
        "\n",
        "`CLIP_MODE_TEXT_PROMPT`: The text prompt of what the image is supposed to be\n",
        "\n",
        "`CLIP_GUIDANCE_PROMPT`: Not needed, but you can additionally also tell CLIP what the image is supposed to be.\n",
        "\n",
        "`CLIP_MODE_STEPS`: How many iterations the generation should have.\n",
        "\n",
        "`CLIP_MODE_SCALE`: How much influence CLIP should have over the image generation\n",
        "\n",
        "`CLIP_MODE_NUM_CUTOUTS`: How many cuts each iteration should have. Higher cuts give finer details\n",
        "\n",
        "*MODE: IMG2IMG SETTINGS*\n",
        "\n",
        "`IMG_PROMPT`: Text prompt of what the image is supposed to be\n",
        "\n",
        "`INIT_IMAGE`: The starting image that the text prompt will draw over. Leave this blank if you want a canvas painter within colab\n",
        "\n",
        "`INIT_STRENGTH`: How much influence the IMG_PROMPT has over the final image\n",
        "\n",
        "*MODE: Inpainting SETTINGS*\n",
        "\n",
        "`INPAINT_PROMPT`: Text prompt of what the inpaint image is supposed to be\n",
        "\n",
        "`INPAINT_IMAGE`: The image that you want to edit\n",
        "\n",
        "`MASK_IMAGE`: What you want to edit out and regenerate. Leave this blank for an editor within colab\n",
        "\n",
        "`INPAINT_STRENGTH`: How much influence the text prompt has over the final image\n",
        "\n",
        "Adding on the other settings later"
      ],
      "metadata": {
        "id": "W40HZzowJt2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Legacy Version (My old code, in case the new one is buggy)"
      ],
      "metadata": {
        "id": "k_QJheXS5BD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorg Template\n",
        "\n",
        "class Manager:\n",
        "  def __init__(self):\n",
        "    # self.pipeline_assigner = {}\n",
        "    pass\n",
        "\n",
        "  class Diffusion:\n",
        "\n",
        "    def get_settings():\n",
        "      pass\n",
        "\n",
        "    def set_settings():\n",
        "      pass\n",
        "\n",
        "    def patch_nsfw():\n",
        "      import shutil\n",
        "      import os\n",
        "      os.remove('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      if ENABLE_NSFW_FILTER:\n",
        "        shutil.copyfile(f'/content/safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "      else:\n",
        "        shutil.copyfile(f'/content/safety_checker_patched.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "\n",
        "    def make_pipe():\n",
        "      pass\n",
        "\n",
        "    def cache_pipe():\n",
        "      pass\n",
        "\n",
        "    def clear_pipe():\n",
        "      pass\n",
        "\n",
        "    class Upscaler:\n",
        "      pass\n",
        "\n",
        "    class Scheduler:\n",
        "      pass\n",
        "\n",
        "    class Methods:\n",
        "      class Prompt:\n",
        "        pass\n",
        "\n",
        "      class ImgToImg:\n",
        "        pass\n",
        "\n",
        "      class Inpaintin:\n",
        "        pass\n",
        "\n",
        "\n",
        "class Colab:\n",
        "  def clean_env():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  class Images:\n",
        "    def resize_image():\n",
        "      pass\n",
        "\n",
        "    def suggest_resolution():\n",
        "      pass\n",
        "\n",
        "  class Library:\n",
        "    def get_imports(requesting_library):\n",
        "      pass\n",
        "\n",
        "  class Downloader:\n",
        "    def fetch_bytes(url_or_path):\n",
        "      if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "          from urllib.request import urlopen \n",
        "          return urlopen(url_or_path) \n",
        "      return open(url_or_path, 'r')\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "# Need to clean up code, it's a mess\n",
        "global LAST_INIT\n",
        "global LAST_VRAM\n",
        "global LAST_MODE\n",
        "global LAST_MODEL_ID\n",
        "global LAST_ENABLE_NSFW_FILTER\n",
        "global LAST_DIFFUSERS_VERSION\n",
        "global INIT_IMAGE\n",
        "global INPAINT_IMAGE\n",
        "global MASK_IMAGE\n",
        "\n",
        "def clean_env():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "\n",
        "def patch_nsfw():\n",
        "  import shutil\n",
        "  import os\n",
        "  os.remove('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "  if ENABLE_NSFW_FILTER:\n",
        "    shutil.copyfile(f'/content/safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "  else:\n",
        "    shutil.copyfile(f'/content/safety_checker_patched.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward(self, x, context=None, mask=None):\n",
        "\n",
        "    import math\n",
        "    from torch import einsum\n",
        "    try:\n",
        "      from einops import rearrange\n",
        "    except ModuleNotFoundError:\n",
        "      !pip install einops\n",
        "      from einops import rearrange\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    import torch\n",
        "    batch_size, sequence_length, dim = x.shape\n",
        "\n",
        "    h = self.heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    context = context if context is not None else x\n",
        "    k = self.to_k(context)\n",
        "    v = self.to_v(context)\n",
        "    del context, x\n",
        "\n",
        "    q = self.reshape_heads_to_batch_dim(q)\n",
        "    k = self.reshape_heads_to_batch_dim(k)\n",
        "    v = self.reshape_heads_to_batch_dim(v)\n",
        "\n",
        "    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device)\n",
        "\n",
        "    stats = torch.cuda.memory_stats(q.device)\n",
        "    mem_total = torch.cuda.get_device_properties(0).total_memory\n",
        "    mem_active = stats['active_bytes.all.current']\n",
        "    mem_free = mem_total - mem_active\n",
        "\n",
        "    mem_required = q.shape[0] * q.shape[1] * k.shape[1] * 4 * 2.5\n",
        "    steps = 1\n",
        "\n",
        "    if mem_required > mem_free:\n",
        "        steps = 2**(math.ceil(math.log(mem_required / mem_free, 2)))\n",
        "\n",
        "    slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "    for i in range(0, q.shape[1], slice_size):\n",
        "        end = i + slice_size\n",
        "        s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n",
        "        s1 *= self.scale\n",
        "\n",
        "        s2 = s1.softmax(dim=-1)\n",
        "        del s1\n",
        "\n",
        "        r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "        del s2\n",
        "\n",
        "    del q, k, v\n",
        "\n",
        "    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "    del r1\n",
        "\n",
        "    return self.to_out(r2)\n",
        "\n",
        "def optimize_attention(model):\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, CrossAttention):\n",
        "            module.forward = types.MethodType(forward, module)\n",
        "\n",
        "def make_pipe():\n",
        "  # TODO: Cache pipes and clean this. Very messy right now\n",
        "  global LOW_VRAM_PATCH\n",
        "  global pipe\n",
        "  pipe = None\n",
        "  clean_env()\n",
        "  if MODE == \"IMG2IMG\":\n",
        "    if LOW_VRAM_PATCH:\n",
        "      try:\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    else:\n",
        "      try:\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "  elif MODE == \"Inpainting\":\n",
        "    if LOW_VRAM_PATCH:\n",
        "      try:\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    else:\n",
        "      try:\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo 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 | huggingface-cli login\n",
        "        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "  elif LOW_VRAM_PATCH:\n",
        "      try:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "      except Exception:\n",
        "        !pip install transformers\n",
        "        # try:\n",
        "        #   with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "        #     key = f.read().decode('utf-8').split(':')\n",
        "        # except OSError as e:\n",
        "        #   print(e)\n",
        "        huggingface_username = 'x90'\n",
        "        huggingface_token = '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'\n",
        "        !echo $huggingface_token | huggingface-cli login\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "        del pipe.vae.encoder\n",
        "  else:\n",
        "    try:\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      !pip install transformers\n",
        "      try:\n",
        "        with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "          key = f.read().decode('utf-8').split(':')\n",
        "      except OSError as e:\n",
        "        print(e)\n",
        "      huggingface_username = 'x90'\n",
        "      huggingface_token = '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'\n",
        "      !echo $huggingface_token | huggingface-cli login\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "      del pipe.vae.encoder\n",
        "  if VRAM_OVER_SPEED:\n",
        "    print(\"Creating pipe optimizations\")\n",
        "    pipe.enable_attention_slicing()\n",
        "    optimize_attention(pipe.unet)\n",
        "\n",
        "def make_scheduler():\n",
        "  if SCHEDULER == 'default':\n",
        "    pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  elif SCHEDULER == 'pndm':\n",
        "    pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  elif SCHEDULER == 'k-lms':\n",
        "    pipe.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "  elif SCHEDULER == 'ddim':\n",
        "    pipe.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "\n",
        "def make_image():\n",
        "  # Clean this\n",
        "  gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "  global pipe\n",
        "  global INIT_IMAGE\n",
        "  global INPAINT_IMAGE\n",
        "  global MASK_IMAGE\n",
        "  if MODE == \"Inpainting\":\n",
        "    print(\"Inpainting Mode\")\n",
        "    print(\"Init Image (automatically resized to match user input)\")\n",
        "    INPAINT_IMAGE = INPAINT_IMAGE.resize((WIDTH, HEIGHT))\n",
        "    MASK_IMAGE = MASK_IMAGE.resize((WIDTH, HEIGHT))\n",
        "    display(INPAINT_IMAGE)\n",
        "    display(MASK_IMAGE)\n",
        "\n",
        "    inpaint_image = INPAINT_IMAGE\n",
        "    mask_image = MASK_IMAGE\n",
        "    # init_image = INIT_IMAGE.convert(\"RGB\"))\n",
        "    # \n",
        "    if SCHEDULER == 'ddim':\n",
        "      try:\n",
        "        image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "    else:\n",
        "      try:\n",
        "        image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, init_image=inpaint_image, mask_image=mask_image, strength=INPAINT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "      except UnboundLocalError or NameError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "        clean_env()\n",
        "        try:\n",
        "          image = None\n",
        "        except Exception:\n",
        "          pass\n",
        "        raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "  elif MODE == \"IMG2IMG\":\n",
        "    print(\"Init Mode\")\n",
        "    print(\"Init Image (automatically resized to match user input)\")\n",
        "    INIT_IMAGE = INIT_IMAGE.resize((WIDTH, HEIGHT))\n",
        "    display(INIT_IMAGE)\n",
        "    def preprocess(image):\n",
        "      w, h = image.size\n",
        "      w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "      image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "      image = np.array(image).astype(np.float32) / 255.0\n",
        "      image = image[None].transpose(0, 3, 1, 2)\n",
        "      image = torch.from_numpy(image)\n",
        "      return 2.*image - 1.\n",
        "\n",
        "    init_image = preprocess(INIT_IMAGE.convert(\"RGB\"))\n",
        "    # init_image = INIT_IMAGE.convert(\"RGB\"))\n",
        "    if SCHEDULER == 'ddim':\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "    else:\n",
        "      try:\n",
        "        image = pipe(prompt=PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(prompt=PROMPT, num_inference_steps=STEPS, init_image=init_image, strength=INIT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "      except UnboundLocalError or NameError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "        clean_env()\n",
        "        try:\n",
        "          image = None\n",
        "        except Exception:\n",
        "          pass\n",
        "        raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "  else:\n",
        "    if SCHEDULER == 'ddim':\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "    else:\n",
        "      try:\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "      except IndexError:\n",
        "        try:\n",
        "          image = pipe(PROMPT, num_inference_steps=STEPS-1, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        except UnboundLocalError or NameError:\n",
        "          make_pipe()\n",
        "          make_scheduler()\n",
        "          make_image()\n",
        "      except UnboundLocalError or NameError or TypeError:\n",
        "        make_pipe()\n",
        "        make_scheduler()\n",
        "        make_image\n",
        "      except RuntimeError as e:\n",
        "        print(e)\n",
        "        clean_env()\n",
        "        try:\n",
        "          image = None\n",
        "        except Exception:\n",
        "          pass\n",
        "        raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "  return image\n",
        "\n",
        "\n",
        "def diffusers_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  try:\n",
        "    with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "      key = f.read().decode('utf-8').split(':')\n",
        "  except OSError as e:\n",
        "    print(e)\n",
        "    \n",
        "  huggingface_username = 'x90'\n",
        "  huggingface_token = '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'\n",
        "  global LAST_INIT, LAST_VRAM, LAST_ENABLE_NSFW_FILTER, LAST_MODEL_ID, LAST_DIFFUSERS_VERSION\n",
        "  LAST_MODE = MODE\n",
        "  LAST_VRAM = LOW_VRAM_PATCH\n",
        "  LAST_ENABLE_NSFW_FILTER = ENABLE_NSFW_FILTER\n",
        "  LAST_MODEL_ID = MODEL_ID\n",
        "  LAST_DIFFUSERS_VERSION = DIFFUSERS_VERSION\n",
        "  try: \n",
        "    !git lfs install\n",
        "    !GIT_LFS_SKIP_SMUDGE=0\n",
        "    # This will take a while\n",
        "    !pip install transformers\n",
        "    !git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/$MODEL_ID\n",
        "    if DIFFUSERS_VERSION == 'latest':\n",
        "      !pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "    else:\n",
        "      !pip install -U git+https://github.com/huggingface/diffusers.git@$DIFFUSERS_VERSION\n",
        "\n",
        "    # Back up original NSFW file\n",
        "    !cp /usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py /content/safety_checker.py\n",
        "    !cp /content/safety_checker.py /content/safety_checker_patched.py\n",
        "    with open(f'/content/safety_checker_patched.py','r') as unpatched_file:\n",
        "      patch = unpatched_file.read().replace('for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):','#for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):').replace('if has_nsfw_concept:','# if has_nsfw_concept:').replace('images[idx] = np.zeros(images[idx].shape)  # black image', '# images[idx] = np.zeros(images[idx].shape)  # black image').replace(\"Potential NSFW content was detected in one or more images. A black image will be returned instead.\",\"Potential NSFW content was detected in one or more images. It's patched out, no actions were taken.\").replace(\" Try again with a different prompt and/or seed.\",\"\")\n",
        "    with open(f'/content/safety_checker_patched.py','w') as file:\n",
        "      file.write(patch)\n",
        "    patch_nsfw()\n",
        "    \n",
        "    # make sure you're logged in with `huggingface-cli login`\n",
        "\n",
        "    !mkdir diffusers_output\n",
        "    !pip install pytorch-pretrained-bert\n",
        "    !pip install spacy ftfy\n",
        "    !python -m spacy download en\n",
        "    !pip install scipy\n",
        "    !echo $huggingface_token | huggingface-cli login\n",
        "  except OSError as e:\n",
        "    raise e\n",
        "  except BaseException as e:\n",
        "    raise e\n",
        "  finally:\n",
        "    if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "    print(\"Setup complete.\")\n",
        "    try:\n",
        "      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "      from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "    except ModuleNotFoundError or ImportError:\n",
        "      diffusers_install()\n",
        "      from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "      from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "\n",
        "\n",
        "def GFPGAN_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  if not os.path.exists('/content/GFPGAN'):\n",
        "    !git clone https://github.com/TencentARC/GFPGAN.git\n",
        "    %cd GFPGAN\n",
        "    !pip install basicsr\n",
        "    !pip install facexlib\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    !pip install realesrgan  \n",
        "    !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "    %cd /content/\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "    \n",
        "\n",
        "def ESRGAN_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  import os\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    import subprocess, os\n",
        "    print(subprocess.run(['git','clone','https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('Real-ESRGAN')\n",
        "    print(subprocess.run(['git','reset', '--hard','2a5afd04a0e43956d1640db00d3a528ca5972fd2'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    print(subprocess.run(['pip','install','-r','/content/Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    os.chdir('/content/')\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "  %cd Real-ESRGAN\n",
        "  from realesrgan import RealESRGAN\n",
        "  clear_output()\n",
        "  device = torch.device('cuda')\n",
        "  global UPSCALE_AMOUNT\n",
        "  \n",
        "  if not os.path.exists(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):\n",
        "    def closest_value(input_list, input_value):\n",
        "      difference = lambda input_list : abs(input_list - input_value)\n",
        "      res = min(input_list, key=difference)\n",
        "      return res\n",
        "    nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "    print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "    UPSCALE_AMOUNT = nearest_value\n",
        "\n",
        "  model = RealESRGAN(device, scale = UPSCALE_AMOUNT)\n",
        "  model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "  %cd /content/\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "\n",
        "def CodeFormer_install():\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  if not os.path.exists('/content/CodeFormer'):\n",
        "    %cd /content\n",
        "    !git clone https://github.com/sczhou/CodeFormer.git\n",
        "    %cd CodeFormer\n",
        "    !pip install -r requirements.txt\n",
        "    !python basicsr/setup.py develop\n",
        "    !python scripts/download_pretrained_models.py facelib\n",
        "    !python scripts/download_pretrained_models.py CodeFormer\n",
        "    !mkdir temp\n",
        "    !mkdir results\n",
        "    %cd /content/\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "\n",
        "# If settings changed, repopulate\n",
        "def populate():\n",
        "  global LAST_INIT\n",
        "  global LAST_MODE\n",
        "  global LAST_VRAM\n",
        "  global LAST_ENABLE_NSFW_FILTER\n",
        "  global LAST_MODEL_ID\n",
        "  global LAST_DIFFUSERS_VERSION\n",
        "  global pipe\n",
        "  pipe = None\n",
        "  clean_env()\n",
        "  if DIFFUSERS_VERSION != LAST_DIFFUSERS_VERSION:\n",
        "    !yes | pip uninstall diffusers\n",
        "  if LAST_MODEL_ID != MODEL_ID or DIFFUSERS_VERSION != LAST_DIFFUSERS_VERSION:\n",
        "    print(\"Setting up for new model..\")\n",
        "    diffusers_install()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  print(\"Patching NSFW...\")\n",
        "  patch_nsfw()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  print(\"Making Pipe...\")\n",
        "  make_pipe()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  print(\"Injecting scheduler...\")\n",
        "  make_scheduler()\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  LAST_MODEL_ID = MODEL_ID\n",
        "  LAST_MODE = MODE\n",
        "  LAST_VRAM = LOW_VRAM_PATCH\n",
        "  LAST_ENABLE_NSFW_FILTER = ENABLE_NSFW_FILTER\n",
        "\n",
        "\n",
        "# Diffuse Function\n",
        "def upscale(image):\n",
        "    try:\n",
        "      from realesrgan import RealESRGAN\n",
        "    except ModuleNotFoundError:\n",
        "      if not os.path.exists('/content/Real-ESRGAN'):\n",
        "        ESRGAN_install()\n",
        "        %cd /content/Real-ESRGAN\n",
        "        from realesrgan import RealESRGAN\n",
        "        %cd /content\n",
        "    device = torch.device('cuda')\n",
        "    model = RealESRGAN(device, scale = UPSCALE_AMOUNT)\n",
        "    try:\n",
        "      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "    except FileNotFoundError:\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "      !wget https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth -O /content/Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "      model.load_weights(f'/content/Real-ESRGAN/weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "\n",
        "\n",
        "# Diffusion start\n",
        "def diffuse_run():\n",
        "  # Can be cleaned quite a bit\n",
        "    global SEED\n",
        "    global pipe\n",
        "    if ORIG_SEED == 0:\n",
        "      if iteration is 0:\n",
        "        SEED = random.randint(0,sys.maxsize)\n",
        "      if iteration is not 0 and not KEEP_SEED:\n",
        "        SEED += 1\n",
        "    else:\n",
        "      if iteration > 0 and not KEEP_SEED:\n",
        "        SEED += 1\n",
        "    gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    clean_env()\n",
        "    \n",
        "    try:\n",
        "      image = make_image()\n",
        "    except NameError or TypeError:\n",
        "      make_pipe()\n",
        "      make_scheduler()\n",
        "      image = make_image()\n",
        "    except RuntimeError as e:\n",
        "      print(e)\n",
        "      clean_env()\n",
        "      try:\n",
        "        image = None\n",
        "      except Exception:\n",
        "        pass\n",
        "      raise SystemExit('\\33[33mUsing too much VRAM, lower your settings.\\33[0m')\n",
        "    display(image)\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "    clean_env()\n",
        "    if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "      print('Face Enhancing and Upscaling... ')\n",
        "      %cd GFPGAN\n",
        "      try:\n",
        "        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s $UPSCALE_AMOUNT --bg_upsampler realesrgan\n",
        "      except FileNotFoundError:\n",
        "        ESRGAN_install()\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      print(f'Moving enhanced image to {OUTDIR}')\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      shutil.move(f'{OUTDIR}/restored_imgs/{filename}', filedir)\n",
        "      try:\n",
        "        if DELETE_ORIGINALS:\n",
        "          os.remove(old_filedir)\n",
        "      except FileNotFoundError:\n",
        "        pass\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(image)\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      try:\n",
        "        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      except NameError:\n",
        "        filedir = f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        os.remove(old_filedir)\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"GFPGAN + Enhanced ESRGAN\":\n",
        "      print('Face Enhancing... ')\n",
        "      %cd GFPGAN\n",
        "      try:\n",
        "        !python /content/GFPGAN/inference_gfpgan.py -i $filedir -o $OUTDIR -v 1.3 -s 1 --bg_upsampler realesrgan\n",
        "      except FileNotFoundError:\n",
        "        ESRGAN_install()\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "      %cd ..\n",
        "      shutil.copy(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      clean_env()\n",
        "      print('Upscaling... ')\n",
        "      sr_image = upscale(PIL.Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      try:\n",
        "        filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      except NameError:\n",
        "        filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"CodeFormer\":\n",
        "      print(\"Face enhancing and Upscaling... \")\n",
        "      # It was behaving weird, hence why I am doing this the weird way\n",
        "      try:\n",
        "        !rm rm /content/CodeFormer/temp/*\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      except Exception as e:\n",
        "        os.makedirs('/content/CodeFormer/temp/')\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      %cd /content/CodeFormer/\n",
        "      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale $UPSCALE_AMOUNT --bg_upsampler realesrgan\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', filedir)\n",
        "      os.remove(f'/content/CodeFormer/temp/{filename}')\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      %cd /content\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(filedir))\n",
        "      clean_env()\n",
        "    if IMAGE_UPSCALER == \"CodeFormer + Enhanced ESRGAN\":\n",
        "      print(\"Face enhancing... \")\n",
        "      try:\n",
        "        !rm /content/CodeFormer/temp/*\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      except Exception as e:\n",
        "        os.makedirs('/content/CodeFormer/temp/')\n",
        "        !cp $filedir /content/CodeFormer/temp/\n",
        "      %cd /content/CodeFormer/\n",
        "      !python inference_codeformer.py --w $CODEFORMER_FIDELITY --test_path /content/CodeFormer/temp --upscale 1 --bg_upsampler realesrgan\n",
        "      os.remove(f'/content/CodeFormer/temp/{filename}')\n",
        "      shutil.copyfile(f'/content/CodeFormer/results/temp_{CODEFORMER_FIDELITY}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "      %cd /content\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(PIL.Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      clean_env()\n",
        "      print(\"Upscaling... \")\n",
        "      sr_image = upscale(PIL.Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(sr_image)\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'\n",
        "      sr_image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "      clean_env()\n",
        "    if int(SHARPEN_AMOUNT) != 0:\n",
        "      def sharpenImage(image, samples=1):\n",
        "        im = image\n",
        "        for i in range(samples):\n",
        "            im = im.filter(ImageFilter.SHARPEN)\n",
        "        return im\n",
        "      print(f\"Sharpening diffusion result with {SHARPEN_AMOUNT} passes.\\n\")\n",
        "      image = sharpenImage(PIL.Image.open(filedir), SHARPEN_AMOUNT)\n",
        "      if not SKIP_PREVIEW:\n",
        "        display(image)\n",
        "      old_filedir = filedir\n",
        "      filedir = f'{filedir.strip(\".png\")}_sharpened_{SHARPEN_AMOUNT}.png'\n",
        "      image.save(filedir)\n",
        "      if DELETE_ORIGINALS:\n",
        "        try:\n",
        "          os.remove(old_filedir)\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "  stop_its = False\n",
        "  %cd /content/\n",
        "  #@title Render Images\n",
        "  MODE = \"PROMPT\" #@param [\"PROMPT\",\"IMG2IMG\",\"Inpainting\"]\n",
        "  #@markdown `MODE` Select what mode you want to use <br>\n",
        "  PROMPT = \"A young woman wearing a hat, greg rutkowski, artgerm, trending on artstation, cinematic animation still, by lois van baarle, ilya kuvshinov, metahuman\" #@param {type:'string'}\n",
        "  #@markdown `PROMPT` The text prompt that is needed for all modes <br>\n",
        "  PROMPT_FILE = '' #@param {type: 'string'}\n",
        "  #@markdown `PROMPT_FILE` is a text file that contains a prompt per line. <br>\n",
        "\n",
        "  #@markdown ---\n",
        "  \n",
        "  #@markdown <b>Init Image Setup (IMG2IMG)</b><br>\n",
        "  #@markdown Still a work in progress, might be buggy<br>\n",
        "\n",
        "  INIT_IMAGE = \"https://raw.githubusercontent.com/dblunk88/txt2imghd/master/character_with_hat.jpg\" #@param {type: 'string'}\n",
        "  #@markdown `INIT_IMAGE`: Can be a local file, URL, or empty (to make your own in colab)<br>\n",
        "  INIT_STRENGTH = 0.92 #@param{type:\"slider\", min:0.01, max:1, step:0.01}\n",
        "  #@markdown `INIT_STRENGTH`: The <B>LOWER</B> this values is, the more strength the init file has on the final output\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>INPAINT Setup</b><br>\n",
        "  #@markdown Still a work in progress, might be buggy<br>\n",
        "  #@markdown `INPAINT_IMAGE` can be a local file or URL<br>\n",
        "  #@markdown `MASK_IMAGE` can be a local file, URL, or empty. If empty, you get to draw your mask within the colab\n",
        "  INPAINT_IMAGE = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\" #@param {type:'string'}\n",
        "  MASK_IMAGE = \"\" #@param {type:'string'}\n",
        "  INPAINT_STRENGTH = 0.96 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>GENERAL SETUP</b><br>\n",
        "  #@markdown Most of these settings are used for all modes\n",
        "  STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "  SCHEDULER = 'default' #@param [\"default\", \"pndm\", \"k-lms\", \"ddim\"]\n",
        "  DDIM_ETA = 0.77 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "  #@markdown Getting good results with `ddim` and `DDIM_ETA` at 0.9\n",
        "  #@markdown Diffusion steps determine the quality of the final image\n",
        "  SEED = 0 #@param {type:'integer'}\n",
        "  #@markdown The seed used for the generation. Leave at `0` for random.\n",
        "  KEEP_SEED = False #@param {type:'boolean'}\n",
        "  #@markdown Will force the program to keep the same seed throughout the iterations.\n",
        "  NUM_ITERS = 3 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "  RUN_FOREVER = False #@param {type:\"boolean\"}\n",
        "  #@markdown Number of iterations for a given prompt.\n",
        "  WIDTH = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "  HEIGHT = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "  SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "  #@markdown The CFG scale determines how closely a generation follows the prompt, or improvisation. Higher values will try to adhear to your prompt.<br>\n",
        "  PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "  #@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>\n",
        "  SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "  USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "  DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>POST PROCESSING</b><br>\n",
        "  #@markdown `IMAGE_UPSCALER`: May not work with the Tesla T4. GFP is pretty good at faces and the enhanced Real-ESRGAN is a pretty good uspcaler. If both is selected, then GFPGAN will act as a face enhancer and Real-ESRGAN will act as the upscaler. Same applies with Codeformer, which seems to be a little more mild than GFP.<br>Recommendations: GFPGAN if you only have faces or People. ESRGAN if you have no people in your prompt. Both if you have people and other things in your prompt<br>Note: ESRGAN only support 2x, 4x, and 8x, if any other value is selected, it will pick the nearest value\n",
        "  IMAGE_UPSCALER = \"CodeFormer + Enhanced ESRGAN\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\"]\n",
        "  UPSCALE_AMOUNT = 2 #@param {type:\"raw\"}\n",
        "  #@markdown `CODEFORMER_FIDELITY`: Only applies if the upscaler includes Codeformer. Balance the quality (lower number) and fidelity (higher number)<br>\n",
        "  CODEFORMER_FIDELITY = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "  #@markdown `SHARPEN_AMOUNT`: Select 0 to turn it off\n",
        "  SHARPEN_AMOUNT = 1 #@param{type:'slider', min:0, max:3, step:1}\n",
        "  DELETE_ORIGINALS = False #@param{type:'boolean'}\n",
        "  #@markdown `SKIP_PREVIEW`: Clicking this might help with connection issues (especially on mobile). It will only show the original result, not the improvements\n",
        "  SKIP_PREVIEW = False #@param{type:'boolean'}\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  #@markdown <b>SETUP</b><br>\n",
        "  MODEL_ID = \"CompVis/stable-diffusion-v1-4\" #@param [\"CompVis/stable-diffusion-v1-4\", \"CompVis/stable-diffusion-v1-3\",\"CompVis/stable-diffusion-v1-2\",\"CompVis/stable-diffusion-v1-1\",\"hakurei/waifu-diffusion\"]\n",
        "  model_id = MODEL_ID\n",
        "  DIFFUSERS_VERSION = 'latest' #@param [\"latest\",\"f3937bc8f3667772c9f1428b66f0c44b6087b04d\"]\n",
        "  LOW_VRAM_PATCH = False #@param {type:\"boolean\"}\n",
        "  #@markdown `LOW_VRAM_PATCH`: Click this if you have CUDA out of memory errors with low settings. If you check this you may be tied to using this setting until the next session restart since it patches various files. <br> This should also speed up iterations but could output lower quality content<br>\n",
        "  \n",
        "  VRAM_OVER_SPEED = True #@param {type:\"boolean\"}\n",
        "  #@markdown `VRAM_OVER_SPEED`: Some more optimizations. This option will prioritize lower VRAM usage over overall speed. Haven't tested this extensively but this is a big boi button. Crank that resolution up in autocast to see the effects<br>\n",
        "  \n",
        "  ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "  #@markdown `ENABLE_NSFW_FILTER`: Will ENABLE the NSFW filter. If you want uncensored results, do not click that. Needs a session restart as of right now if changed (fixing this)<br>\n",
        "  \n",
        "  CLEAR_SETUP_LOG = True #@param{type: 'boolean'}\n",
        "  #markdown Clear the setup log after installation compeltes.\n",
        "\n",
        "  #@markdown ---\n",
        "\n",
        "  ##@markdown <b>Advanced Options</b><br>\n",
        "  ##@markdown If you don't know what you are doing, do NOT touch this<br>\n",
        "\n",
        "\n",
        "  #     pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  # elif SCHEDULER == 'pndm':\n",
        "  #   pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000, skip_prk_steps=True)\n",
        "  # elif SCHEDULER == 'k-lms':\n",
        "  #   pipe.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "  # elif SCHEDULER == 'ddim':\n",
        "  #   pipe.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "  \n",
        "  # Need to clean up imports\n",
        "  try:\n",
        "    import os, torch, gc\n",
        "  except ValueError:\n",
        "    !yes | pip uninstall numpy\n",
        "    !pip install -U numpy\n",
        "    import os, torch, gc\n",
        "  from PIL import Image\n",
        "  import random, time, shutil, sys\n",
        "  from contextlib import contextmanager, nullcontext\n",
        "  from torch import autocast\n",
        "  from IPython.display import clear_output\n",
        "  import numpy as np\n",
        "  import PIL.Image\n",
        "  import PIL\n",
        "  from PIL import ImageFilter\n",
        "  # Google Drive\n",
        "  if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "    !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "  if USE_DRIVE_FOR_PICS:\n",
        "    OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "  else:\n",
        "    OUTDIR = '/content/diffusers_output'\n",
        "  try:\n",
        "    os.makedirs(OUTDIR)\n",
        "  except FileExistsError:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "    from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "  except ModuleNotFoundError or ImportError:\n",
        "    diffusers_install()\n",
        "    from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "    from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipeline\n",
        "  # Decide precision and set variables\n",
        "  precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "  ORIG_SEED = SEED\n",
        "  DRIVE_PIC_DIR = DRIVE_PIC_DIR.strip()\n",
        "\n",
        "  \n",
        "\n",
        "  # Enable third-party widgets\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "\n",
        "  \n",
        "  # Split this into a function\n",
        "  #  INPAINT_IMAGE = \"https://pbs.twimg.com/media/FbwYUfXaMAATXEj?format=jpg&name=large\" #@param {type:'string'}\n",
        "  # MASK_IMAGE = \"\" #@param {type:'string'}\n",
        "  # INPAINT_STRENGTH = 0.96 #@param {type:\"slider\", min:0, max:1, step:0.01} \n",
        "  if MODE == \"Inpainting\":\n",
        "    def download_image(url):\n",
        "      import requests\n",
        "      from io import BytesIO\n",
        "      response = requests.get(url)\n",
        "      return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    if 'http' in INPAINT_IMAGE:\n",
        "      INPAINT_IMAGE = download_image(INPAINT_IMAGE).resize((WIDTH, HEIGHT))\n",
        "    elif INPAINT_IMAGE:\n",
        "      INPAINT_IMAGE = PIL.Image.open(INPAINT_IMAGE)\n",
        "    if 'http' in MASK_IMAGE:\n",
        "      MASK_IMAGE = download_image(MASK_IMAGE).resize((WIDTH, HEIGHT))\n",
        "    elif MASK_IMAGE:\n",
        "      MASK_IMAGE = PIL.Image.open(MASK_IMAGE)\n",
        "    INPAINT_IMAGE.save(\"init.jpg\")\n",
        "    if not MASK_IMAGE:\n",
        "      import requests\n",
        "      from io import BytesIO\n",
        "      def draw(filename='drawing.png', color=\"white\", w=256, h=256, line_width=50,loop=False, init_img=\"init.jpg\"):\n",
        "        filename=\"init.jpg\"\n",
        "        import google\n",
        "        from IPython.display import HTML\n",
        "        from base64 import b64decode\n",
        "        import os\n",
        "        import shutil\n",
        "        import uuid\n",
        "        COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "\n",
        "        def moveToExt(filename:str) -> str:\n",
        "          if not os.path.exists(filename):\n",
        "            print(\"Image file not found\")\n",
        "            return None\n",
        "          \n",
        "          target = os.path.basename(filename)\n",
        "          target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "          \n",
        "          shutil.copyfile(filename,target)\n",
        "          print(\"moved to ext\")\n",
        "          return target\n",
        "        real_filename = os.path.realpath(filename)\n",
        "        html_filename = real_filename\n",
        "        html_real_filename = html_filename\n",
        "        if os.path.exists(real_filename):\n",
        "          html_real_filename = moveToExt(real_filename)\n",
        "          html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "          \n",
        "\n",
        "        canvas_html = f\"\"\"\n",
        "      <canvas width={w} height={h}></canvas>\n",
        "\n",
        "      <div class=\"slidecontainer\">\n",
        "      <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "        <input type=\"range\" min=\"1\" max=\"100\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "      </div>\n",
        "\n",
        "      <div>\n",
        "        <button id=\"loadImage\">Reload from disk</button>\n",
        "        <button id=\"reset\">Reset</button>\n",
        "        <button id=\"save\">Save</button>\n",
        "      </div>\n",
        "      <script>\n",
        "\n",
        "      function loadImage(url) {{\n",
        "      return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "      }}\n",
        "        \n",
        "        \n",
        "        var canvas = document.querySelector('canvas')\n",
        "        var ctx = canvas.getContext('2d')\n",
        "        ctx.lineWidth = {line_width};\n",
        "        \n",
        "        ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "        ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "\n",
        "        var slider = document.getElementById(\"lineWidth\");\n",
        "        slider.oninput = function() {{\n",
        "          ctx.lineWidth = this.value;\n",
        "          lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "        }}\n",
        "\n",
        "\n",
        "        function updateStroke(event){{\n",
        "            ctx.strokeStyle = event.target.value\n",
        "        }}\n",
        "        function updateBG(event){{\n",
        "            ctx.fillStyle = event.target.value\n",
        "        }}\n",
        "        \n",
        "        \n",
        "        var clear_button = document.querySelector('#reset')\n",
        "        var reload_img_button = document.querySelector('#loadImage')\n",
        "        \n",
        "        var button = document.querySelector('#save')\n",
        "\n",
        "        var mouse = {{x: 0, y: 0}}\n",
        "        canvas.addEventListener('mousemove', function(e) {{\n",
        "          mouse.x = e.pageX - this.offsetLeft\n",
        "          mouse.y = e.pageY - this.offsetTop\n",
        "        }})\n",
        "        canvas.onmousedown = ()=>{{\n",
        "          ctx.beginPath()\n",
        "          ctx.moveTo(mouse.x, mouse.y)\n",
        "          canvas.addEventListener('mousemove', onPaint)\n",
        "        }}\n",
        "        canvas.onmouseup = ()=>{{\n",
        "          canvas.removeEventListener('mousemove', onPaint)\n",
        "        }}\n",
        "        var onPaint = ()=>{{\n",
        "          ctx.lineTo(mouse.x, mouse.y)\n",
        "          ctx.stroke()\n",
        "        }}\n",
        "        reload_img_button.onclick = async ()=>{{\n",
        "          console.log(\"Reloading Image {html_filename}\")\n",
        "          let img = await loadImage('{html_filename}'); \n",
        "          console.log(\"Loaded image\")\n",
        "          ctx.drawImage(img, 0, 0);\n",
        "\n",
        "        }}\n",
        "        reload_img_button.click()\n",
        "      \n",
        "        clear_button.onclick = ()=>{{\n",
        "            console.log('Clearing Screen')\n",
        "            ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "            ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          }}\n",
        "          canvas.addEventListener('load', function() {{\n",
        "          console.log('All assets are loaded')\n",
        "        }})\n",
        "        var data = new Promise(resolve=>{{\n",
        "          button.onclick = ()=>{{\n",
        "\n",
        "            var c = ctx\n",
        "            var imageData = ctx.getImageData(0,0, {w}, {h});\n",
        "            var pixel = imageData.data;\n",
        "            var r=0, g=1, b=2,a=3;\n",
        "          for (var p = 0; p<pixel.length; p+=4)\n",
        "          {{\n",
        "            if (\n",
        "                pixel[p+r] != 255 &&\n",
        "                pixel[p+g] != 255 &&\n",
        "                pixel[p+b] != 255) \n",
        "            {{pixel[p+r] =0; pixel[p+g]=0; pixel[p+b]=0}}\n",
        "          }}\n",
        "\n",
        "          c.putImageData(imageData,0,0);\n",
        "            resolve(canvas.toDataURL('image/png'))\n",
        "          }}\n",
        "          \n",
        "        }})\n",
        "        \n",
        "        \n",
        "      </script>\n",
        "      \"\"\"\n",
        "        print(HTML)\n",
        "        display(HTML(canvas_html))\n",
        "        print(\"Evaluating JS\")\n",
        "        \n",
        "        data = google.colab.output.eval_js(\"data\")\n",
        "        if data:\n",
        "          print(\"Saving Sketch\")  \n",
        "          binary = b64decode(data.split(',')[1])\n",
        "          # filename = html_real_filename if loop else filename\n",
        "          with open(\"init_mask.png\", 'wb') as f:\n",
        "            f.write(binary)\n",
        "          #return len(binary)\n",
        "\n",
        "\n",
        "\n",
        "      draw(filename = \"init_mask.png\", w=WIDTH, h=HEIGHT)\n",
        "      MASK_IMAGE = PIL.Image.open('init_mask.png')\n",
        "  if MODE == \"IMG2IMG\":\n",
        "    if 'http' in INIT_IMAGE:\n",
        "      import requests\n",
        "      from io import BytesIO\n",
        "      response = requests.get(INIT_IMAGE)\n",
        "      init_image = PIL.Image.open(BytesIO(response.content))\n",
        "      INIT_IMAGE = init_image\n",
        "    else:\n",
        "      if INIT_IMAGE == None or INIT_IMAGE == \"\":\n",
        "        if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "        print(\"No init image found, go ahead and draw your own below this text\")\n",
        "        def draw(filename='drawing.png', color=\"black\", bg_color=\"transparent\",w=256, h=256, line_width=1,loop=False):\n",
        "          import google\n",
        "          from IPython.display import HTML\n",
        "          from base64 import b64decode\n",
        "          import os\n",
        "          import shutil\n",
        "          import uuid\n",
        "          COLAB_HTML_ROOT = \"/usr/local/share/jupyter/nbextensions/google.colab/\"\n",
        "\n",
        "          def moveToExt(filename:str) -> str:\n",
        "            if not os.path.exists(filename):\n",
        "              print(\"Image file not found\")\n",
        "              return None\n",
        "            \n",
        "            target = os.path.basename(filename)\n",
        "            target = os.path.join(COLAB_HTML_ROOT, str(uuid.uuid4()) + target)\n",
        "            \n",
        "            shutil.copyfile(filename,target)\n",
        "            print(\"moved to ext\")\n",
        "            return target\n",
        "          real_filename = os.path.realpath(filename)\n",
        "          html_filename = real_filename\n",
        "          html_real_filename = html_filename\n",
        "          if os.path.exists(real_filename):\n",
        "            html_real_filename = moveToExt(real_filename)\n",
        "            html_filename = html_real_filename.replace(\"/usr/local/share/jupyter\",\"\")\n",
        "            \n",
        "\n",
        "          canvas_html = f\"\"\"\n",
        "        <canvas width={w} height={h}></canvas>\n",
        "        <div>\n",
        "          <label for=\"strokeColor\">Stroke</label>\n",
        "          <input type=\"color\" value=\"{color}\" id=\"strokeColor\">\n",
        "        \n",
        "          <label for=\"bgColor\">Background</label>\n",
        "          <input type=\"color\" value=\"{bg_color}\" id=\"bgColor\">\n",
        "        </div>\n",
        "        <div class=\"slidecontainer\">\n",
        "        <label for=\"lineWidth\" id=\"lineWidthLabel\">{line_width}px</label>\n",
        "          <input type=\"range\" min=\"1\" max=\"35\" value=\"1\" class=\"slider\" id=\"lineWidth\">\n",
        "        </div>\n",
        "\n",
        "        <div>\n",
        "          <button id=\"loadImage\">Reload from disk</button>\n",
        "          <button id=\"reset\">Reset</button>\n",
        "          <button id=\"save\">Save</button>\n",
        "          <button id=\"exit\">Exit</button>\n",
        "        </div>\n",
        "        <script>\n",
        "\n",
        "        function loadImage(url) {{\n",
        "        return new Promise(r => {{ let i = new Image(); i.onload = (() => r(i)); i.src = url; }});\n",
        "      }}\n",
        "          \n",
        "          \n",
        "          var canvas = document.querySelector('canvas')\n",
        "          var ctx = canvas.getContext('2d')\n",
        "          ctx.lineWidth = {line_width}\n",
        "          ctx.fillStyle = \"{bg_color}\";\n",
        "          \n",
        "          ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "          ctx.strokeStyle = \"{color}\";\n",
        "\n",
        "          var strokeColor = document.querySelector('#strokeColor')\n",
        "          var bgColor = document.querySelector('#bgColor')\n",
        "\n",
        "          var slider = document.getElementById(\"lineWidth\");\n",
        "          slider.oninput = function() {{\n",
        "            ctx.lineWidth = this.value;\n",
        "            lineWidthLabel.innerHTML = `${{this.value}}px`\n",
        "          }}\n",
        "\n",
        "          function updateStroke(event){{\n",
        "              ctx.strokeStyle = event.target.value\n",
        "          }}\n",
        "          function updateBG(event){{\n",
        "              ctx.fillStyle = event.target.value\n",
        "          }}\n",
        "          \n",
        "          bgColor.addEventListener(\"change\", updateBG, false);\n",
        "          strokeColor.addEventListener(\"change\", updateStroke, false);\n",
        "          \n",
        "          var clear_button = document.querySelector('#reset')\n",
        "          var reload_img_button = document.querySelector('#loadImage')\n",
        "          var button = document.querySelector('#save')\n",
        "          var exit_button = document.querySelector('#exit')\n",
        "\n",
        "          var mouse = {{x: 0, y: 0}}\n",
        "          canvas.addEventListener('mousemove', function(e) {{\n",
        "            mouse.x = e.pageX - this.offsetLeft\n",
        "            mouse.y = e.pageY - this.offsetTop\n",
        "          }})\n",
        "          canvas.onmousedown = ()=>{{\n",
        "            ctx.beginPath()\n",
        "            ctx.moveTo(mouse.x, mouse.y)\n",
        "            canvas.addEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          canvas.onmouseup = ()=>{{\n",
        "            canvas.removeEventListener('mousemove', onPaint)\n",
        "          }}\n",
        "          var onPaint = ()=>{{\n",
        "            ctx.lineTo(mouse.x, mouse.y)\n",
        "            ctx.stroke()\n",
        "          }}\n",
        "          reload_img_button.onclick = async ()=>{{\n",
        "            console.log(\"Reloading Image {html_filename}\")\n",
        "            let img = await loadImage('{html_filename}'); \n",
        "            console.log(\"Loaded image\")\n",
        "            ctx.drawImage(img, 0, 0);\n",
        "\n",
        "          }}\n",
        "          \n",
        "          clear_button.onclick = ()=>{{\n",
        "              console.log('Clearing Screen')\n",
        "              ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "              ctx.fillRect(0, 0, canvas.width, canvas.height);\n",
        "            }}\n",
        "            canvas.addEventListener('load', function() {{\n",
        "            console.log('All assets are loaded')\n",
        "          }})\n",
        "          var data = new Promise(resolve=>{{\n",
        "            button.onclick = ()=>{{\n",
        "              resolve(canvas.toDataURL('image/png'))\n",
        "            }}\n",
        "            exit_button.onclick = ()=>{{\n",
        "            resolve()\n",
        "          }}\n",
        "            \n",
        "          }})\n",
        "          \n",
        "          // window.onload = async ()=>{{\n",
        "          //   console.log(\"loaded\")\n",
        "          //   let img = await loadImage('{html_filename}');  \n",
        "          //   ctx.drawImage(img, 0, 0);\n",
        "          // }}\n",
        "          \n",
        "          \n",
        "        </script>\n",
        "        \"\"\"\n",
        "          print(HTML)\n",
        "          display(HTML(canvas_html))\n",
        "          print(\"Evaluating JS\")\n",
        "          \n",
        "          data = google.colab.output.eval_js(\"data\")\n",
        "          if data:\n",
        "            print(\"Saving Sketch\")  \n",
        "            binary = b64decode(data.split(',')[1])\n",
        "            # filename = html_real_filename if loop else filename\n",
        "            with open(filename, 'wb') as f:\n",
        "              f.write(binary)\n",
        "            #return len(binary)\n",
        "        \n",
        "\n",
        "\n",
        "        draw(filename = \"custom_image.png\", w=WIDTH, h=HEIGHT, bg_color=\"blue\", line_width=10)\n",
        "        INIT_IMAGE = \"/content/custom_image.png\"\n",
        "      INIT_IMAGE = PIL.Image.open(INIT_IMAGE)\n",
        "\n",
        "  # Check if upscalers are installed\n",
        "  if \"GFPGAN\" in IMAGE_UPSCALER:\n",
        "    GFPGAN_install()\n",
        "  if \"ESRGAN\"in IMAGE_UPSCALER:\n",
        "    ESRGAN_install()\n",
        "  if \"CodeFormer\" in IMAGE_UPSCALER:\n",
        "    CodeFormer_install()\n",
        "\n",
        "  try:\n",
        "    if MODE != LAST_MODE or LOW_VRAM_PATCH != LAST_VRAM or ENABLE_NSFW_FILTER != LAST_ENABLE_NSFW_FILTER or LAST_MODEL_ID != MODEL_ID or LAST_DIFFUSERS_VERSION != DIFFUSERS_VERSION:\n",
        "      print(\"Pipe specific settings have changed, repopulating pipe with new settings...\")\n",
        "      populate()\n",
        "  except NameError:\n",
        "    LAST_MODE = MODE\n",
        "    LAST_VRAM = LOW_VRAM_PATCH\n",
        "    LAST_ENABLE_NSFW_FILTER = ENABLE_NSFW_FILTER\n",
        "    LAST_MODEL_ID = MODEL_ID\n",
        "    LAST_DIFFUSERS_VERSION = DIFFUSERS_VERSION\n",
        "  if CLEAR_SETUP_LOG: from IPython.display import clear_output; clear_output()\n",
        "  # Make this into a function and create a function for anything repititive\n",
        "  PROMPTS = []\n",
        "  if PROMPT_FILE not in ['','none']:\n",
        "      try:\n",
        "          with open(PROMPT_FILE, \"r\") as f:\n",
        "              PROMPTS = f.read().splitlines()\n",
        "      except OSError as e:\n",
        "          raise e\n",
        "\n",
        "  if PROMPT not in ['', 'none']:\n",
        "      PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    with precision_scope(\"cuda\"):\n",
        "        if RUN_FOREVER:\n",
        "          while True:\n",
        "            for pi in PROMPTS:\n",
        "              PROMPT = pi\n",
        "              print(OUTDIR)\n",
        "              if SAVE_PROMPT_DETAILS:\n",
        "                  epoch_time = int(time.time())\n",
        "                  try:\n",
        "                    with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\nETA: {DDIM_ETA}\\nINIT_STRENGTH: {INIT_STRENGTH}\\nSCHEDULER: {SCHEDULER}')\n",
        "                  except FileNotFoundError:\n",
        "                    os.makedirs(OUTDIR)\n",
        "              for iteration in range(NUM_ITERS):\n",
        "                clean_env()\n",
        "                try:\n",
        "                  diffuse_run()\n",
        "                except KeyboardInterrupt:\n",
        "                  stop_its = True\n",
        "                  import os\n",
        "                  clean_env()\n",
        "                  try:\n",
        "                    image = None\n",
        "                  except Exception:\n",
        "                    pass\n",
        "                  raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                except TypeError:\n",
        "                  make_pipe()\n",
        "                  make_scheduler()\n",
        "                  try:\n",
        "                    diffuse_run()\n",
        "                  except KeyboardInterrupt:\n",
        "                    stop_its = True\n",
        "                    import os\n",
        "                    clean_env()\n",
        "                    try:\n",
        "                      image = None\n",
        "                    except Exception:\n",
        "                      pass\n",
        "                    raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                clean_env()\n",
        "        else:\n",
        "          for pi in PROMPTS:\n",
        "              PROMPT = pi\n",
        "              print(OUTDIR)\n",
        "              if SAVE_PROMPT_DETAILS:\n",
        "                  epoch_time = int(time.time())\n",
        "                  try:\n",
        "                    with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\nETA:{DDIM_ETA}')\n",
        "                  except FileNotFoundError:\n",
        "                    os.makedirs(OUTDIR)\n",
        "              for iteration in range(NUM_ITERS):\n",
        "                clean_env()\n",
        "                try:\n",
        "                  diffuse_run()\n",
        "                except KeyboardInterrupt:\n",
        "                  stop_its = True\n",
        "                  import os\n",
        "                  clean_env()\n",
        "                  try:\n",
        "                    image = None\n",
        "                  except Exception:\n",
        "                    pass\n",
        "                  raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                except TypeError:\n",
        "                  make_pipe()\n",
        "                  make_scheduler()\n",
        "                  try:\n",
        "                    diffuse_run()\n",
        "                  except KeyboardInterrupt:\n",
        "                    stop_its = True\n",
        "                    import os\n",
        "                    clean_env()\n",
        "                    try:\n",
        "                      image = None\n",
        "                    except Exception:\n",
        "                      pass\n",
        "                    raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                clean_env()\n",
        "        clean_env()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "br4zBt3d5Nhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-cd15ZYjLqf"
      },
      "source": [
        "## Diffuser Experiments (run through the Diffuser setup first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jdSQSFlRjONj"
      },
      "outputs": [],
      "source": [
        "#@title Modifier Tester\n",
        "#@markdown `MODIFIER_FILE`: location of a text file with a list of modifiers seperated by newline.<br>You will need to upload it. Then right click on the file in colab and then click \"copy path\" (If you can't find it, click on the folder icon on the left pane). Then paste it in the box. See modifier_examples.txt for an example (if you're lazy, just edit the file. It will populate AFTER the first run)<br>\n",
        "#@markdown `BASE_PROMPT`: the prompt against which the modifiers will be tested<br>\n",
        "with open('modifier_examples.txt','w') as file:\n",
        "  file.write('Canon\\nNikon\\nPanasonic\\nSony\\nDigital Painting\\nMatte Painting\\nDrawing from a 5 year old\\nPasta Art\\nI made this while on acid')\n",
        "\n",
        "MODIFIER_FILE = \"/content/modifier_examples.txt\" #@param {type:'string'}\n",
        "BASE_PROMPT = \"A dog playing in a field\" #@param {type:'string'}\n",
        "STEPS = 50 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 42 #@param {type:'integer'}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SEED = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "\n",
        "OUTDIR = '/content/experiments'\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "with open(MODIFIER_FILE) as file:\n",
        "  for line in file.readlines():\n",
        "    line = line.strip()\n",
        "    PROMPT = f\"{BASE_PROMPT}, {line}\"\n",
        "    print(f\"Running: {PROMPT}\")\n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=SEED)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    except FileNotFoundError:\n",
        "      !mkdir $OUTDIR\n",
        "      image.save(f'{OUTDIR}/modifier_{line}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q9upICRkMQcx"
      },
      "outputs": [],
      "source": [
        "#@title Randomizer, aka: I feel lucky/Fuck my shit up\n",
        "import os\n",
        "import random\n",
        "WORDS_AMOUNT = 30 #@param {type:\"integer\"}\n",
        "STEPS = 90 #@param {type:\"slider\", min:5, max:500, step:5}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.7 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "\n",
        "if not os.path.exists('words.txt'):\n",
        "  !wget https://gist.githubusercontent.com/h3xx/1976236/raw/bbabb412261386673eff521dddbe1dc815373b1d/wiki-100k.txt -O words.txt\n",
        "with open('words.txt') as file:\n",
        "  words = file.readlines()\n",
        "  prompt = \"\"\n",
        "  for iteration in range(WORDS_AMOUNT):\n",
        "    again = True\n",
        "    while again:\n",
        "      word = random.choice(words).strip()\n",
        "      if not '#' in word:\n",
        "        again = False\n",
        "    prompt += f'{random.choice(words).strip()}, '\n",
        "prompt = prompt[:-2]\n",
        "print(f'Prompt: {prompt}')\n",
        "with precision_scope(\"cuda\"):\n",
        "  seed = torch.Generator(\"cuda\").manual_seed(random.randint(0,4294967295))\n",
        "  image = pipe(prompt, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0] \n",
        "  display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcnY9hUsy76f"
      },
      "source": [
        "#TXT2IMG Method (Needs fixing, diffusers right now is my priority)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BKZThsccD4Da"
      },
      "outputs": [],
      "source": [
        "#@title Huggingface Login\n",
        "from getpass import getpass\n",
        "\n",
        "huggingface_username = '' #@param {type:\"string\"}\n",
        "huggingface_token = '' #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-HMZ9IiRDs8Q"
      },
      "outputs": [],
      "source": [
        "#@title TXT2IMG Setup\n",
        "import os\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "condacolab.install()\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "import os\n",
        "if not os.path.isdir(root_code):\n",
        "  !mkdir $root_code\n",
        "%cd $root_code\n",
        "!git clone https://github.com/DamascusGit/stable-diffusion.git\n",
        "!mamba env update -n base -f stable-diffusion/environment.yaml\n",
        "!pip install torchmetrics==0.6.0\n",
        "!pip install kornia==0.6)\n",
        "if not os.path.isdir(root_model):\n",
        "  !mkdir $root_model\n",
        "%cd $root_model\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "# Will take a long time\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "!mkdir /content/txt2img_output\n",
        "%cd $code_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9QnhfmAM0t-X"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "import os\n",
        "from torch import autocast\n",
        "torch.cuda.empty_cache()\n",
        "PROMPT = \"matte potrait of a young cyberpunk woman as a Disney Princess, full-frame, complex picture, intricate, fine details, vogue, trending on artstation, artgerm, greg manchess, studio ghibli, Disney, Star Wars\" #@param {type:'string'}\n",
        "STEPS = 160 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 0 #@param {type:'integer'}\n",
        "NUM_ITERS = 6 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "IMAGE_UPSCALER = True #@param {type:\"boolean\"}\n",
        "UPSCALE_AMOUNT = \"2\" #@param [\"2\",\"4\", \"8\"]\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "if IMAGE_UPSCALER:\n",
        "  def upscale(image):\n",
        "    sr_image = model.predict(np.array(image))\n",
        "    return sr_image\n",
        "  if not os.path.exists('/content/Real-ESRGAN'):\n",
        "    !git clone https://github.com/sberbank-ai/Real-ESRGAN\n",
        "    !pip install -r Real-ESRGAN/requirements.txt\n",
        "    !gdown https://drive.google.com/uc?id=1pG2S3sYvSaO0V0B8QPOl1RapPHpUGOaV -O Real-ESRGAN/weights/RealESRGAN_x2.pth\n",
        "    !gdown https://drive.google.com/uc?id=1SGHdZAln4en65_NQeQY9UjchtkEF9f5F -O Real-ESRGAN/weights/RealESRGAN_x4.pth\n",
        "    !gdown https://drive.google.com/uc?id=1mT9ewx86PSrc43b-ax47l1E2UzR7Ln4j -O Real-ESRGAN/weights/RealESRGAN_x8.pth\n",
        "  %cd /content/Real-ESRGAN\n",
        "  from realesrgan import RealESRGAN\n",
        "  from PIL import Image\n",
        "  import numpy as np\n",
        "  import torch\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "  model = RealESRGAN(device, scale = int(UPSCALE_AMOUNT))\n",
        "  model.load_weights(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth')\n",
        "  %cd /content/\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "else:\n",
        "  OUTDIR = '/content/diffusers_output'\n",
        "epoch_time = int(time.time())\n",
        "if SAVE_PROMPT_DETAILS:\n",
        "  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "        file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "with precision_scope(\"cuda\"):\n",
        "  for iteration in range(NUM_ITERS):\n",
        "    \n",
        "    if ORIG_SEED == 0:\n",
        "      rand_num = random.randint(0,4294967295)\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "    else:\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    try:\n",
        "      print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    except NameError:\n",
        "      print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    \n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "    except NameError:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "    print('Upscaling... ')\n",
        "    sr_image = upscale(image)\n",
        "    display(sr_image)\n",
        "    try:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "    except NameError:\n",
        "      sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "#@markdown If you're using the `low VRAM patch` you <b>HAVE</b> to use `autocast`<br>\n",
        "#@markdown `Out of Memory error`: If the VRAM stacks right after execution, sometimes it helps waiting for a minute before running it again. Looking at ways to force it to clear the VRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQeX4o971T_U"
      },
      "source": [
        "#How to install offline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFdp-FFg1cao"
      },
      "source": [
        "https://rentry.org/SDInstallation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TeIWggi6TGH0",
        "OPyJJ2z-RJB7",
        "Z-cd15ZYjLqf",
        "HQeX4o971T_U"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
